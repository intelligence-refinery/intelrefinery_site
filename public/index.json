[{"categories":["Natural language processing"],"contents":"\r\r\r\r\r\r\r\r\r\n\rtextrank\rtextrank is a R package that uses the textrank algorithm to 1) rank sentences by their “importance” and 2) identify keywords that appear more frequently in the test. The workflow presented here is a combination of posts by Jan Wijffels and Emil Hvitfeldt.\nWorflow\rGet text\rLet’s start by grabbing the article text used in the post by Emil Hvitfeldt, which is about a newly released Fitbit for kids. Here, I’m using a slightly different rvest command to scrape the article, as the instructions provided scraped a lot of extra website boilerplate text that muddied the waters, so to speak, and screwed up the textrank results.\n## Import libraries\rlibrary(tidyverse)\rlibrary(tidytext)\rlibrary(textrank)\rlibrary(rvest)\rurl \u0026lt;- \u0026quot;http://time.com/5196761/fitbit-ace-kids-fitness-tracker/\u0026quot;\rarticle \u0026lt;- read_html(url) %\u0026gt;%\rhtml_nodes(\u0026#39;p\u0026#39;) %\u0026gt;%\rhtml_text()\rarticle \u0026lt;- paste(article, collapse = \u0026quot;\\n\u0026quot;)\rprint(article)\r## [1] \u0026quot;Fitbit is launching a new fitness tracker designed for children called the Fitbit Ace, which will go on sale for $99.95 in the second quarter of this year.\\nThe Fitbit Ace looks a lot like the company’s Alta tracker, but with a few child-friendly tweaks. The most important of which is Fitbit’s new family account option, which gives parents control over how their child uses their tracker and is compliant with the Children’s Online Privacy Protection Act, or COPPA. Parents must approve who their child can connect with via the Fitbit app and can view their kid’s activity progress and sleep trends, the latter of which can help them manage their children’s bedtimes.\\nLike many of Fitbit’s other products, the Fitbit Ace can automatically track steps, monitor active minutes, and remind kids to move when they’ve been still for too long. But while Fitbit’s default move goal is 30 minutes for adult users, the Ace’s will be 60 minutes, in line with the World Health Organization’s recommendation that children between the ages of five and 17 get an hour of daily physical activity per day. Fitbit says the tracker is designed for children eight years old and up.\\nFitbit will also be introducing a Family Faceoff feature that lets kids compete in a five-day step challenge against the other members of their family account. The app also will reward children with in-app badges for achieving their health goals. Fitbit’s new child-friendly fitness band will be available in blue and purple, is showerproof, and should last for five days on a single charge.\\nThe Ace launch is part of Fitbit’s broader goal of branching out to new audiences. The company also announced a new smartwatch on Tuesday called the Versa, which is being positioned as an everyday smartwatch rather than a fitness-only device or sports watch, like some of the company’s other products.\\nAbove all else, the Ace is an effort to get children up and moving. The Centers for Disease Control and Prevention report that the percentage of children and adolescents affected by obesity has more than tripled since the 1970’s. But parents who want to encourage their children to move already have several less expensive options to choose from. Garmin’s $79.99 Vivofit Jr. 2, for example, comes in themed skins like these Minnie Mouse and Star Wars versions, while the wristband entices kids to move by reflecting their fitness achievements in an accompanying smartphone game. The $39.99 Nabi Compete, meanwhile, is sold in pairs so that family members can work together to achieve movement milestones.\\nContact us at editors@time.com.\u0026quot;\r\rPart-of-speech tagging\rAs the textrank algorithm measures similiarity between sentences by the extend of word overlap between them, it is important to compare them only in terms of the most informative words. This is done by part-of-speech (POS) tagging, to identify nouns, verbs and adjectives.\nHere we will use the udpipe package (Github repo), which provides language-agnostic tokenization, tagging, lemmatization and dependency parsing of raw text.\n## Import library\rlibrary(udpipe)\r## Create model\rtagger \u0026lt;- udpipe_download_model(\u0026quot;english\u0026quot;)\rtagger \u0026lt;- udpipe_load_model(tagger$file_model)\r## Annotate text\rarticle \u0026lt;- udpipe_annotate(tagger, article)\rLet’s look at the results:\narticle \u0026lt;- as.data.frame(article)\rarticle %\u0026gt;% head(10) %\u0026gt;% select(-sentence) %\u0026gt;% kable() %\u0026gt;%\rkable_styling(bootstrap_options = c(\u0026quot;striped\u0026quot;, \u0026quot;hover\u0026quot;), full_width = F) %\u0026gt;%\rscroll_box(width = \u0026quot;100%\u0026quot;)\r\r\rdoc_id\r\rparagraph_id\r\rsentence_id\r\rtoken_id\r\rtoken\r\rlemma\r\rupos\r\rxpos\r\rfeats\r\rhead_token_id\r\rdep_rel\r\rdeps\r\rmisc\r\r\r\r\r\rdoc1\r\r1\r\r1\r\r1\r\rFitbit\r\rFitbit\r\rPROPN\r\rNNP\r\rNumber=Sing\r\r3\r\rnsubj\r\rNA\r\rNA\r\r\r\rdoc1\r\r1\r\r1\r\r2\r\ris\r\rbe\r\rAUX\r\rVBZ\r\rMood=Ind|Number=Sing|Person=3|Tense=Pres|VerbForm=Fin\r\r3\r\raux\r\rNA\r\rNA\r\r\r\rdoc1\r\r1\r\r1\r\r3\r\rlaunching\r\rlaunch\r\rVERB\r\rVBG\r\rTense=Pres|VerbForm=Part\r\r0\r\rroot\r\rNA\r\rNA\r\r\r\rdoc1\r\r1\r\r1\r\r4\r\ra\r\ra\r\rDET\r\rDT\r\rDefinite=Ind|PronType=Art\r\r7\r\rdet\r\rNA\r\rNA\r\r\r\rdoc1\r\r1\r\r1\r\r5\r\rnew\r\rnew\r\rADJ\r\rJJ\r\rDegree=Pos\r\r7\r\ramod\r\rNA\r\rNA\r\r\r\rdoc1\r\r1\r\r1\r\r6\r\rfitness\r\rfitness\r\rNOUN\r\rNN\r\rNumber=Sing\r\r7\r\rcompound\r\rNA\r\rNA\r\r\r\rdoc1\r\r1\r\r1\r\r7\r\rtracker\r\rtracker\r\rNOUN\r\rNN\r\rNumber=Sing\r\r3\r\robj\r\rNA\r\rNA\r\r\r\rdoc1\r\r1\r\r1\r\r8\r\rdesigned\r\rdesign\r\rVERB\r\rVBN\r\rTense=Past|VerbForm=Part\r\r7\r\racl\r\rNA\r\rNA\r\r\r\rdoc1\r\r1\r\r1\r\r9\r\rfor\r\rfor\r\rADP\r\rIN\r\rNA\r\r10\r\rcase\r\rNA\r\rNA\r\r\r\rdoc1\r\r1\r\r1\r\r10\r\rchildren\r\rchild\r\rNOUN\r\rNNS\r\rNumber=Plur\r\r8\r\robl\r\rNA\r\rNA\r\r\r\r\r\r\rExtract keywords\r\r“In order to find relevant keywords, the textrank algorithm constructs a word network. This network is constructed by looking which words follow one another. A link is set up between two words if they follow one another, the link gets a higher weight if these 2 words occur more frequenctly next to each other in the text.”\n\rThe only input to textrank_keywords() is the lemmatized words, with the option of filtering for only certain types of words, such as nouns, verbs and adjectives:\narticle_keywords \u0026lt;- textrank_keywords(article$lemma, relevant = article$upos %in% c(\u0026quot;NOUN\u0026quot;, \u0026quot;VERB\u0026quot;, \u0026quot;ADJ\u0026quot;))\rarticle_keywords$keywords %\u0026gt;%\rsubset(freq \u0026gt; 1) %\u0026gt;%\rkable()\r\r\r\rkeyword\r\rngram\r\rfreq\r\r\r\r\r\rchild\r\r1\r\r12\r\r\r\rtracker\r\r1\r\r4\r\r\r\rkid\r\r1\r\r4\r\r\r\rmove\r\r1\r\r4\r\r\r\rparent\r\r1\r\r3\r\r\r\rapp\r\r1\r\r3\r\r\r\rother\r\r1\r\r3\r\r\r\rgoal\r\r1\r\r3\r\r\r\rfitness\r\r1\r\r3\r\r\r\ryear\r\r1\r\r2\r\r\r\rfamily-account\r\r2\r\r2\r\r\r\raccount\r\r1\r\r2\r\r\r\ractivity\r\r1\r\r2\r\r\r\rstep\r\r1\r\r2\r\r\r\rfamily\r\r1\r\r2\r\r\r\rnew\r\r1\r\r2\r\r\r\r\r\rRank sentences\rLoosely, the textrank algorithm computes similarities between a pair of sentences by the number of words that they have in common.\nThe textrank_sentences() function takes in two inputs 1:\nA dataframe with sentences:\nsentences \u0026lt;- unique(article[, c(\u0026quot;sentence_id\u0026quot;, \u0026quot;sentence\u0026quot;)])\rA dataframe with words which are part of each sentence:\nterminology \u0026lt;- article %\u0026gt;% filter(upos %in% c(\u0026quot;NOUN\u0026quot;, \u0026quot;VERB\u0026quot;, \u0026quot;ADJ\u0026quot;)) %\u0026gt;%\rselect(sentence_id, lemma) \rBased on these two dataframes, the function calculates the pairwise distance between each sentence by computing how many terms are overlapping, in terms of Jaccard distance. These pairwise distances among the sentences are next passed on to Google’s pagerank algorithm to identify the most relevant sentences.\ntr \u0026lt;- textrank_sentences(data = sentences, terminology = terminology)\rLet’s see how the sentences rank:\nkable(tr[[\u0026#39;sentences\u0026#39;]] %\u0026gt;% arrange(-textrank))\r\r\r\rtextrank_id\r\rsentence\r\rtextrank\r\r\r\r\r\r1\r\rFitbit is launching a new fitness tracker designed for children called the Fitbit Ace, which will go on sale for $99.95 in the second quarter of this year.\r\r0.0784357\r\r\r\r15\r\rAbove all else, the Ace is an effort to get children up and moving.\r\r0.0672520\r\r\r\r4\r\rThe most important of which is Fitbit’s new family account option, which gives parents control over how their child uses their tracker and is compliant with the Children’s Online Privacy Protection Act, or COPPA.\r\r0.0652963\r\r\r\r12\r\rFitbit’s new child-friendly fitness band will be available in blue and purple, is showerproof, and should last for five days on a single charge.\r\r0.0626908\r\r\r\r8\r\rBut while Fitbit’s default move goal is 30 minutes for adult users, the Ace’s will be 60 minutes, in line with the World Health Organization’s recommendation that children between the ages of five and 17 get an hour of daily physical activity per day.\r\r0.0621032\r\r\r\r9\r\rFitbit says the tracker is designed for children eight years old and up.\r\r0.0615568\r\r\r\r17\r\rBut parents who want to encourage their children to move already have several less expensive options to choose from.\r\r0.0591652\r\r\r\r11\r\rThe app also will reward children with in-app badges for achieving their health goals.\r\r0.0588857\r\r\r\r5\r\rParents must approve who their child can connect with via the Fitbit app and can view their kid’s activity progress and sleep trends, the latter of which can help them manage their children’s bedtimes.\r\r0.0578003\r\r\r\r3\r\rAce looks a lot like the company’s Alta tracker, but with a few child-friendly tweaks.\r\r0.0572568\r\r\r\r10\r\rFitbit will also be introducing a Family Faceoff feature that lets kids compete in a five-day step challenge against the other members of their family account.\r\r0.0561673\r\r\r\r14\r\rThe company also announced a new smartwatch on Tuesday called the Versa, which is being positioned as an everyday smartwatch rather than a fitness-only device or sports watch, like some of the company’s other products.\r\r0.0498193\r\r\r\r6\r\rLike many of Fitbit’s other products, the Fitbit\r\r0.0454276\r\r\r\r16\r\rThe Centers for Disease Control and Prevention report that the percentage of children and adolescents affected by obesity has more than tripled since the 1970’s.\r\r0.0387110\r\r\r\r7\r\rAce can automatically track steps, monitor active minutes, and remind kids to move when they’ve been still for too long.\r\r0.0370217\r\r\r\r19\r\rWars versions, while the wristband entices kids to move by reflecting their fitness achievements in an accompanying smartphone game.\r\r0.0351309\r\r\r\r2\r\rThe Fitbit\r\r0.0332401\r\r\r\r13\r\rThe Ace launch is part of Fitbit’s broader goal of branching out to new audiences.\r\r0.0299872\r\r\r\r21\r\rNabi Compete, meanwhile, is sold in pairs so that family members can work together to achieve movement milestones.\r\r0.0209157\r\r\r\r18\r\rGarmin’s $79.99 Vivofit Jr. 2, for example, comes in themed skins like these Minnie Mouse and Star\r\r0.0077121\r\r\r\r20\r\rThe $39.99\r\r0.0077121\r\r\r\r22\r\rContact us at editors@time.com.\r\r0.0077121\r\r\r\r\rMakes sense, but it would be nice to have a human-made summary for comparison.\nAlternatively, we can see what the top five most “informative” sentences are in their order of appearance:\ns \u0026lt;- summary(tr, n = 5, keep.sentence.order = TRUE)\rcat(s, sep = \u0026quot;\\n\\n\u0026quot;)\r## Fitbit is launching a new fitness tracker designed for children called the Fitbit Ace, which will go on sale for $99.95 in the second quarter of this year.\r## ## The most important of which is Fitbit’s new family account option, which gives parents control over how their child uses their tracker and is compliant with the Children’s Online Privacy Protection Act, or COPPA.\r## ## But while Fitbit’s default move goal is 30 minutes for adult users, the Ace’s will be 60 minutes, in line with the World Health Organization’s recommendation that children between the ages of five and 17 get an hour of daily physical activity per day.\r## ## Fitbit’s new child-friendly fitness band will be available in blue and purple, is showerproof, and should last for five days on a single charge.\r## ## Above all else, the Ace is an effort to get children up and moving.\rFinally, we can also visualize the textrank value of sentences by their order of appearance in the text.\ntr[[\u0026quot;sentences\u0026quot;]] %\u0026gt;%\rggplot(aes(textrank_id, textrank, fill = textrank_id)) +\rgeom_col(color=\u0026quot;blue\u0026quot;, fill=rgb(0.1,0.4,0.5,0.7)) +\rtheme_classic() +\rguides(fill = \u0026quot;none\u0026quot;) +\rlabs(x = \u0026quot;Sentence\u0026quot;,\ry = \u0026quot;TextRank score\u0026quot;)\r\r\rAnother example\rLately, I’ve been trying to analyze job descriptions. Let’s see how textrank summarization performs on one for a data scientist position.\nlink \u0026lt;- \u0026quot;https://ca.indeed.com/viewjob?cmp=Slice-Insurance-Technologies-Inc.\u0026amp;t=Data+Scientist\u0026amp;jk=35c9914ee9b0a052\u0026amp;sjdu=vQIlM60yK_PwYat7ToXhk3myaatk1OsAkhQIw2UEit1eiAfw8fAJl6BKuZ5V5P0TgQjo7h2qNGvAZ_80Lr3XxA\u0026amp;tk=1e1nuvuml0np3000\u0026amp;adid=335410488\u0026amp;pub=4a1b367933fd867b19b072952f68dceb\u0026amp;vjs=3\u0026quot;\rdescription \u0026lt;- tryCatch(\ras.character(link) %\u0026gt;% read_html() %\u0026gt;% html_nodes(xpath=\u0026#39;//*[(@id = \u0026quot;jobDescriptionText\u0026quot;)]\u0026#39;) %\u0026gt;% map(xml_contents),\rerror=function(e){NA}\r)\rif (is.null(description)){\rdesc \u0026lt;- NA\r}\rfinal \u0026lt;- tryCatch(\rpaste(as.character(description[[1]]), collapse=\u0026#39; \u0026#39;),\rerror=function(e){NA}\r)\rif (is.null(description)){\rNA\r}\rjob_des \u0026lt;- html_text(read_html(final))\rprint(job_des)\r## [1] \u0026quot;WE are Slice Labs (Slice). We’re building a team of innovators and disruptors to change the insurance experience to meet the demands of an on-demand world. We are disrupting the idea that insurance needs to be a fixed product, with a fixed term, with fixed coverage. We believe it can and should be all digital and on-demand, so customers get only the coverage they need, right when they need it. At Slice, we’re focused on ensuring our products provide a positive, individualized customer experience. Our smaller teams invite broader thinking and problem solving, where nobody is pigeonholed into a predetermined role. We work in an open, supportive, environment that values and promotes inclusiveness, innovation, and collaboration. It’s fast-paced, dynamic and fulfilling.\\n *The Data Science team is key to Slice’s ongoing success. The data we have and the models we build are foundational to our platform and quite literally drive our business. Our work improves the customer experience, grows our market share and drives business outcomes. This is an opportunity to play an active part in delivering our data vision and determining how we get there. All that to say: Data Science work here at Slice is far from theoretical.\\n This ROLE develops and executes Data Science projects across the company. This means hands-on work at all points in the data lifecycle, including data wrangling and mining; feature engineering; model-building and testing; and implementation and communication. Your work will ensure that all our decisions are data-driven - this means that you will have a direct impact on the customer experience by influencing critical decisions on resource deployment and customer engagement. Our immediate applications include marketing analytics, fraud prevention, and risk/value modeling. (Our future applications are boundless!)\\n YOU are passionate about data. You enjoy being able to combine your analytical, technical and business skills in one role. You love solving problems and predicting behaviours. You are a collaborator and a communicator and are energized by working with multidisciplinary teams. You are a hands-on learner and are excited by the thought of moving past theory and examples to real-time data science work.\\n You bring: \\n An effective communication style with an ability to translate “the complex” to “the simple”. You are adept at data visualizations and have experience working with real-time data.4+ years of practical experience using data, models, and common sense to solve tough problems in a collaborative environment. Experience deploying models is a plus!Working experience with Python, including Pandas, NumPy, scikit-learn, NLTK, and Keras/TensorFlow.An understanding of statistical and predictive modeling concepts, machine learning algorithms, clustering and classification techniques.Some exposure to one or more sub-fields of data science, especially GIS/spatial analysis; graph theory/network analysis; or natural language processing (NLP).Working experience with non-Excel BI tools such as Tableau, Looker, Superset, PowerBI, etc.You are comfortable building datasets using traditional relational databases and you’re familiar with alternative databases (noSQL, graph databases) or big data platforms such as Apache Spark.University degree in engineering, applied statistics, data mining, machine learning, mathematics or a related quantitative discipline.*\\n Job Types: Full-time, Permanent\\n Experience:\\n Data Science: 4 years (Required)Location:\\n Ottawa, ON (Preferred)\u0026quot;\rPart-of-speech tagging:\njob_des \u0026lt;- udpipe_annotate(tagger, job_des)\rjob_des \u0026lt;- as.data.frame(job_des)\rjob_des %\u0026gt;% head(10) %\u0026gt;% select(-sentence) %\u0026gt;% kable() %\u0026gt;%\rkable_styling(bootstrap_options = c(\u0026quot;striped\u0026quot;, \u0026quot;hover\u0026quot;), full_width = F) %\u0026gt;%\rscroll_box(width = \u0026quot;100%\u0026quot;)\r\r\rdoc_id\r\rparagraph_id\r\rsentence_id\r\rtoken_id\r\rtoken\r\rlemma\r\rupos\r\rxpos\r\rfeats\r\rhead_token_id\r\rdep_rel\r\rdeps\r\rmisc\r\r\r\r\r\rdoc1\r\r1\r\r1\r\r1\r\rWE\r\rwe\r\rPRON\r\rPRP\r\rCase=Nom|Number=Plur|Person=1|PronType=Prs\r\r4\r\rnsubj\r\rNA\r\rNA\r\r\r\rdoc1\r\r1\r\r1\r\r2\r\rare\r\rbe\r\rAUX\r\rVBP\r\rMood=Ind|Tense=Pres|VerbForm=Fin\r\r4\r\rcop\r\rNA\r\rNA\r\r\r\rdoc1\r\r1\r\r1\r\r3\r\rSlice\r\rslice\r\rPROPN\r\rNNP\r\rNumber=Sing\r\r4\r\rcompound\r\rNA\r\rNA\r\r\r\rdoc1\r\r1\r\r1\r\r4\r\rLabs\r\rlabs\r\rPROPN\r\rNNP\r\rNumber=Sing\r\r0\r\rroot\r\rNA\r\rNA\r\r\r\rdoc1\r\r1\r\r1\r\r5\r\r(\r\r(\r\rPUNCT\r\r-LRB-\r\rNA\r\r6\r\rpunct\r\rNA\r\rSpaceAfter=No\r\r\r\rdoc1\r\r1\r\r1\r\r6\r\rSlice\r\rslice\r\rPROPN\r\rNNP\r\rNumber=Sing\r\r4\r\rappos\r\rNA\r\rSpaceAfter=No\r\r\r\rdoc1\r\r1\r\r1\r\r7\r\r)\r\r)\r\rPUNCT\r\r-RRB-\r\rNA\r\r6\r\rpunct\r\rNA\r\rSpaceAfter=No\r\r\r\rdoc1\r\r1\r\r1\r\r8\r\r.\r\r.\r\rPUNCT\r\r.\r\rNA\r\r4\r\rpunct\r\rNA\r\rNA\r\r\r\rdoc1\r\r1\r\r2\r\r1\r\rWe\r\rwe\r\rPRON\r\rPRP\r\rCase=Nom|Number=Plur|Person=1|PronType=Prs\r\r2\r\rnsubj\r\rNA\r\rSpaceAfter=No\r\r\r\rdoc1\r\r1\r\r2\r\r2\r\r’re\r\r’re\r\rVERB\r\rVBP\r\rMood=Ind|Tense=Pres|VerbForm=Fin\r\r0\r\rroot\r\rNA\r\rNA\r\r\r\r\r\rThe keyword extraction functionality seemed lack-luster for this example too, giving what is essentially a list of most frequently appearing words.\njob_des_keywords \u0026lt;- textrank_keywords(job_des$lemma, relevant = job_des$upos %in% c(\u0026quot;NOUN\u0026quot;, \u0026quot;VERB\u0026quot;, \u0026quot;ADJ\u0026quot;))\rjob_des_keywords$keywords %\u0026gt;%\rsubset(freq \u0026gt; 1) %\u0026gt;%\rkable()\r\r\r\rkeyword\r\rngram\r\rfreq\r\r\r\r\r\rdata\r\r1\r\r12\r\r\r\rexperience\r\r1\r\r8\r\r\r\rwork\r\r1\r\r8\r\r\r\r’re\r\r1\r\r3\r\r\r\rteam\r\r1\r\r3\r\r\r\rdemand\r\r1\r\r3\r\r\r\rcustomer-experience\r\r2\r\r3\r\r\r\rproblem\r\r1\r\r3\r\r\r\rrole\r\r1\r\r3\r\r\r\rbusiness\r\r1\r\r3\r\r\r\rdata-science\r\r2\r\r3\r\r\r\rscience\r\r1\r\r3\r\r\r\rlearn\r\r1\r\r3\r\r\r\rdatabase\r\r1\r\r3\r\r\r\rproduct\r\r1\r\r2\r\r\r\rfix\r\r1\r\r2\r\r\r\rcustomer\r\r1\r\r2\r\r\r\rmean\r\r1\r\r2\r\r\r\rbuilding\r\r1\r\r2\r\r\r\rcommunication\r\r1\r\r2\r\r\r\rmodeling\r\r1\r\r2\r\r\r\rtheory\r\r1\r\r2\r\r\r\rwork-experience\r\r2\r\r2\r\r\r\ranalysis\r\r1\r\r2\r\r\r\rsuch\r\r1\r\r2\r\r\r\r\rFinally, let’s look at the sentence rankings:\nsentences \u0026lt;- unique(job_des[, c(\u0026quot;sentence_id\u0026quot;, \u0026quot;sentence\u0026quot;)])\rterminology \u0026lt;- job_des %\u0026gt;% filter(upos %in% c(\u0026quot;NOUN\u0026quot;, \u0026quot;VERB\u0026quot;, \u0026quot;ADJ\u0026quot;)) %\u0026gt;%\rselect(sentence_id, lemma) tr \u0026lt;- textrank_sentences(data = sentences, terminology = terminology)\rkable(tr[[\u0026#39;sentences\u0026#39;]] %\u0026gt;% arrange(-textrank))\r\r\r\rtextrank_id\r\rsentence\r\rtextrank\r\r\r\r\r\r27\r\rYou are adept at data visualizations and have experience working with real-time data.4+ years of practical experience using data, models, and common sense to solve tough problems in a collaborative environment.\r\r0.0703072\r\r\r\r17\r\rYour work will ensure that all our decisions are data-driven - this means that you will have a direct impact on the customer experience by influencing critical decisions on resource deployment and customer engagement.\r\r0.0611901\r\r\r\r11\r\rOur work improves the customer experience, grows our market share and drives business outcomes.\r\r0.0575264\r\r\r\r16\r\rThis means hands-on work at all points in the data lifecycle, including data wrangling and mining; feature engineering; model-building and testing; and implementation and communication.\r\r0.0567150\r\r\r\r10\r\rThe data we have and the models we build are foundational to our platform and quite literally drive our business.\r\r0.0509835\r\r\r\r24\r\rYou are a hands-on learner and are excited by the thought of moving past theory and examples to real-time data science work.\r\r0.0507275\r\r\r\r30\r\rWorking experience with non-Excel BI tools such as Tableau, Looker, Superset, PowerBI, etc.You are comfortable building datasets using traditional relational databases and you’re familiar with alternative databases (noSQL, graph databases) or big data platforms such as Apache Spark.University degree in engineering, applied statistics, data mining, machine learning, mathematics or a related quantitative discipline.*\r\r0.0445864\r\r\r\r29\r\rWorking experience with Python, including Pandas, NumPy, scikit-learn, NLTK, and Keras/TensorFlow.An understanding of statistical and predictive modeling concepts, machine learning algorithms, clustering and classification techniques.Some exposure to one or more sub-fields of data science, especially GIS/spatial analysis; graph theory/network analysis; or natural language processing (NLP).\r\r0.0434717\r\r\r\r2\r\rWe’re building a team of innovators and disruptors to change the insurance experience to meet the demands of an on-demand world.\r\r0.0430290\r\r\r\r23\r\rYou are a collaborator and a communicator and are energized by working with multidisciplinary teams.\r\r0.0420002\r\r\r\r5\r\rAt Slice, we’re focused on ensuring our products provide a positive, individualized customer experience.\r\r0.0394096\r\r\r\r32\r\rData Science: 4 years (Required)Location:\r\r0.0362884\r\r\r\r28\r\rExperience deploying models is a plus!\r\r0.0350955\r\r\r\r6\r\rOur smaller teams invite broader thinking and problem solving, where nobody is pigeonholed into a predetermined role.\r\r0.0342888\r\r\r\r20\r\rYOU are passionate about data.\r\r0.0341904\r\r\r\r14\r\rScience work here at Slice is far from theoretical.\r\r0.0332422\r\r\r\r7\r\rWe work in an open, supportive, environment that values and promotes inclusiveness, innovation, and collaboration.\r\r0.0307840\r\r\r\r31\r\rJob Types: Full-time, Permanent Experience:\r\r0.0290179\r\r\r\r12\r\rThis is an opportunity to play an active part in delivering our data vision and determining how we get there.\r\r0.0280136\r\r\r\r4\r\rWe believe it can and should be all digital and on-demand, so customers get only the coverage they need, right when they need it.\r\r0.0270914\r\r\r\r18\r\rOur immediate applications include marketing analytics, fraud prevention, and risk/value modeling.\r\r0.0221562\r\r\r\r21\r\rYou enjoy being able to combine your analytical, technical and business skills in one role.\r\r0.0211016\r\r\r\r9\r\r*The Data Science team is key to Slice’s ongoing success.\r\r0.0184101\r\r\r\r3\r\rWe are disrupting the idea that insurance needs to be a fixed product, with a fixed term, with fixed coverage.\r\r0.0182936\r\r\r\r15\r\rThis ROLE develops and executes Data Science projects across the company.\r\r0.0152376\r\r\r\r19\r\r(Our future applications are boundless!)\r\r0.0120358\r\r\r\r22\r\rYou love solving problems and predicting behaviours.\r\r0.0114608\r\r\r\r26\r\rAn effective communication style with an ability to translate “the complex” to “the simple”.\r\r0.0072589\r\r\r\r1\r\rWE are Slice Labs (Slice).\r\r0.0052174\r\r\r\r8\r\rIt’s fast-paced, dynamic and fulfilling.\r\r0.0052174\r\r\r\r13\r\rAll that to say: Data\r\r0.0052174\r\r\r\r25\r\rYou bring:\r\r0.0052174\r\r\r\r33\r\rOttawa, ON (Preferred)\r\r0.0052174\r\r\r\r\rNot bad, the top sentences are the big picture stuff that might give an overall impression of the job.\nAnd, just out of interest:\ntr[[\u0026quot;sentences\u0026quot;]] %\u0026gt;%\rggplot(aes(textrank_id, textrank, fill = textrank_id)) +\rgeom_col(color=\u0026quot;blue\u0026quot;, fill=rgb(0.1,0.4,0.5,0.7)) +\rtheme_classic() +\rguides(fill = \u0026quot;none\u0026quot;) +\rlabs(x = \u0026quot;Sentence\u0026quot;,\ry = \u0026quot;TextRank score\u0026quot;)\r\rConclusion\rtextrank is a nifty little package for simple extractive text summary. However, it is important to think carefully about the type of summary you are aiming for, and whether this type of extractive summaries by the textrank algorithm can accomplish it.\nPros\n\rEasy to use\n\r\rSummaries make sense at a glance\n\r\nCons\n\rHard to know if these summaries really are of good quality, as there are no easy ways to validate the quality of the summaries\n\r\rThe measure of similarities between sentences, namely the number of words that two sentences have in common, is quite simplistic, and so can only produce “summaries” in a certain sense\n\r\r\rpytextrank\r\rgensim\r\rsumma\r\r\r\r\r\r\rhttps://rdrr.io/cran/textrank/man/textrank_sentences.html↩\n\r\r\r","permalink":"/post/extractive-summarization/","tags":["R","Python","Package","textrank","Text summarization"],"title":"Extractive text summarization"},{"categories":["Data science toolbox"],"contents":"       \n --       \n One of the most exciting things about data science is when you get your hands on a new dataset. Oh, the sense of possibilities when opening up a new dataset!\nUnfortunately, before you can get to the fun stuff (though who said that EDA can’t be fun), it’s important to get an idea of its overall structure and potential problems. Here is a round up of our favourite packages for getting acquainted with a dataset while writing a minimum amount of code.\nesquisse: interactive data exploration with ggplot2 If you are really impatient, esquisse is an RStudio addin that launches a point-and-click GUI for absolutely no-code interactive EDA. After drag-and-drop selection of the features that you want to visualize, it not only generates customized beautiful ggplot2 figures but also exports the code so that you can easily replicate them elsewhere.\nFrom its official documentation:\nesquisse can also be used as a module inside your Shiny application.\n dataMaid: quality check of raw data To quickly spot things like missing values, misclassified variables, and erroneous values, I prefer dataMaid for its straight forward combination of metrics and visualizations.\n## Import library library(\u0026#39;dataMaid\u0026#39;) ## Import data raw Telco customer churn dataset raw_df \u0026lt;- read.csv(\u0026quot;https://github.com/treselle-systems/customer_churn_analysis/raw/master/WA_Fn-UseC_-Telco-Customer-Churn.csv\u0026quot;) dataMaid generates a summary report of your dataset in R markdown format, which you can knit together into an PDF or HTML report. For demonstration purposes, I will just show snippets of the interesting parts:\n## Generate report makeDataReport(raw_df, openResult = TRUE, output=\u0026#39;html\u0026#39;, render = TRUE, file = \u0026quot;./auto_eda_report.Rmd\u0026quot;, replace = TRUE, codebook=TRUE) First part of the generated report shows the types of checks performed:\n   Then, we see a summary table of all variables, which provides a helpful quick overview of the data and any potential issues, like the 0.16% missing data in the TotalCharges column.\n   Scrolling down, there are more detailed information on each variable. We see problematic areas such as the customerID column being a key and that the SeniorCitizen column is encoded in 0s and 1s.\n   Also we see that the minimum value of Tenure column is 0, which is problematic and should be removed.\n   Of all the automated EDA packages in R and Python that I have tried so far, dataMaid provides the best once-over, quick-glance view of the whole dataset with a single function. These results are great for focusing the initial data cleaning process.\n autoEDA: quick overview of cleaned data Once I get a (reasonably) clean data set, I want to be able to explore the variables and their relationships with minimal coding (at first). This is where the next two packages come in, which provide varying degrees of flexibility and depth of insights.\nFor the first quick overview, I use the autoEDA package to explore the relationship between all input variables and my target variable of interest, which is Churn in this case. For maximum convenience, this is can be done in a single line of code:\n## Import libraries library(autoEDA) ## Import the same dataset, but with basic cleaning cleaned_df \u0026lt;- read.csv(\u0026quot;https://github.com/nchelaru/data-prep/raw/master/telco_cleaned_yes_no.csv\u0026quot;) ## Correctly format the target variable cleaned_df$Churn \u0026lt;- as.character(cleaned_df$Churn) ## autoEDA autoEDA_results \u0026lt;- autoEDA(cleaned_df, y = \u0026quot;Churn\u0026quot;, returnPlotList = TRUE, verbose = FALSE)  The graphical outputs provided by autoEDA give very quick at-a-glance insights into how various aspects of customer demographics and behaviour relate to whether they churn or not. As there are many plots, one for each variable plus some more, I will show them in a nifty carousel made possible by the slickR package:\n## Import libraries library(svglite) library(lattice) library(ggplot2) library(rvest) library(reshape2) library(dplyr) library(htmlwidgets) library(slickR) ## Create list of autoEDA figures converted to SVG plotsToSVG \u0026lt;- list() i \u0026lt;- 1 for (v in autoEDA_results$plots) { x \u0026lt;- xmlSVG({show(v)}, standalone=TRUE) plotsToSVG[[i]] \u0026lt;- x i \u0026lt;- i +1 } ## Custom function needed to render SVGs in Chrome/Firefox hash_encode_url \u0026lt;- function(url){ gsub(\u0026quot;#\u0026quot;, \u0026quot;%23\u0026quot;, url) } ## Pass list of figures to SlickR s.in \u0026lt;- sapply(plotsToSVG, function(sv){hash_encode_url(paste0(\u0026quot;data:image/svg+xml;utf8,\u0026quot;,as.character(sv)))}) slickR(s.in, slideId = \u0026#39;ex4\u0026#39;,slickOpts = list(dots=T), width = \u0026#39;100%\u0026#39;) #htmlwidget-8f1773b6fe181ad14286 {margin-left:auto;margin-right:auto}  {\"x\":[{\"divName\":\"ex4\",\"divType\":\"img\",\"padding\":\"99%\",\"obj\":[\"data:image/svg+xml;utf8,\\n\\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n 0%\\n \\n \\n 20%\\n \\n \\n 40%\\n \\n \\n 60%\\n \\n \\n \\n \\n \\n \\n \\n \\n \\n No\\n \\n \\n Yes\\n \\n \\n Churn\\n \\n \\n Relative Frequency\\n \\n \\n \\n Churn\\n \\n \\n \\n \\n No\\n \\n \\n Yes\\n \\n \\n Outcome distribution\\n \\n\\n\",\"data:image/svg+xml;utf8,\\n\\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n 0%\\n \\n \\n 25%\\n \\n \\n 50%\\n \\n \\n 75%\\n \\n \\n 100%\\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n No\\n \\n \\n Yes\\n \\n \\n Churn\\n \\n \\n Relative Frequency\\n \\n \\n \\n Gender\\n \\n \\n \\n \\n FEMALE\\n \\n \\n MALE\\n \\n \\n Distribution: Churn By Gender\\n \\n\\n\",\"data:image/svg+xml;utf8,\\n\\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n 0%\\n \\n \\n 25%\\n \\n \\n 50%\\n \\n \\n 75%\\n \\n \\n 100%\\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n No\\n \\n \\n Yes\\n \\n \\n Churn\\n \\n \\n Relative Frequency\\n \\n \\n \\n SeniorCitizen\\n \\n \\n \\n \\n NO\\n \\n \\n YES\\n \\n \\n Distribution: Churn By SeniorCitizen\\n \\n\\n\",\"data:image/svg+xml;utf8,\\n\\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n 0%\\n \\n \\n 25%\\n \\n \\n 50%\\n \\n \\n 75%\\n \\n \\n 100%\\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n No\\n \\n \\n Yes\\n \\n \\n Churn\\n \\n \\n Relative Frequency\\n \\n \\n \\n Partner\\n \\n \\n \\n \\n NO\\n \\n \\n YES\\n \\n \\n Distribution: Churn By Partner\\n \\n\\n\",\"data:image/svg+xml;utf8,\\n\\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n 0%\\n \\n \\n 25%\\n \\n \\n 50%\\n \\n \\n 75%\\n \\n \\n 100%\\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n No\\n \\n \\n Yes\\n \\n \\n Churn\\n \\n \\n Relative Frequency\\n \\n \\n \\n Dependents\\n \\n \\n \\n \\n NO\\n \\n \\n YES\\n \\n \\n Distribution: Churn By Dependents\\n \\n\\n\",\"data:image/svg+xml;utf8,\\n\\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n 0\\n \\n \\n 200\\n \\n \\n 400\\n \\n \\n 600\\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n 0\\n \\n \\n 20\\n \\n \\n 40\\n \\n \\n 60\\n \\n \\n Tenure\\n \\n \\n Frequency\\n \\n \\n \\n Churn\\n \\n \\n \\n \\n No\\n \\n \\n Yes\\n \\n \\n Distribution: Tenure By Churn\\n \\n\\n\",\"data:image/svg+xml;utf8,\\n\\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n 0%\\n \\n \\n 25%\\n \\n \\n 50%\\n \\n \\n 75%\\n \\n \\n 100%\\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n No\\n \\n \\n Yes\\n \\n \\n Churn\\n \\n \\n Relative Frequency\\n \\n \\n \\n PhoneService\\n \\n \\n \\n \\n NO\\n \\n \\n YES\\n \\n \\n Distribution: Churn By PhoneService\\n \\n\\n\",\"data:image/svg+xml;utf8,\\n\\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n 0%\\n \\n \\n 25%\\n \\n \\n 50%\\n \\n \\n 75%\\n \\n \\n 100%\\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n No\\n \\n \\n Yes\\n \\n \\n Churn\\n \\n \\n Relative Frequency\\n \\n \\n \\n MultipleLines\\n \\n \\n \\n \\n NO\\n \\n \\n YES\\n \\n \\n Distribution: Churn By MultipleLines\\n \\n\\n\",\"data:image/svg+xml;utf8,\\n\\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n 0%\\n \\n \\n 25%\\n \\n \\n 50%\\n \\n \\n 75%\\n \\n \\n 100%\\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n No\\n \\n \\n Yes\\n \\n \\n Churn\\n \\n \\n Relative Frequency\\n \\n \\n \\n InternetService\\n \\n \\n \\n \\n \\n DSL\\n \\n \\n FIBER OPTIC\\n \\n \\n NO\\n \\n \\n Distribution: Churn By InternetService\\n \\n\\n\",\"data:image/svg+xml;utf8,\\n\\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n 0%\\n \\n \\n 25%\\n \\n \\n 50%\\n \\n \\n 75%\\n \\n \\n 100%\\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n No\\n \\n \\n Yes\\n \\n \\n Churn\\n \\n \\n Relative Frequency\\n \\n \\n \\n OnlineSecurity\\n \\n \\n \\n \\n NO\\n \\n \\n YES\\n \\n \\n Distribution: Churn By OnlineSecurity\\n \\n\\n\",\"data:image/svg+xml;utf8,\\n\\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n 0%\\n \\n \\n 25%\\n \\n \\n 50%\\n \\n \\n 75%\\n \\n \\n 100%\\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n No\\n \\n \\n Yes\\n \\n \\n Churn\\n \\n \\n Relative Frequency\\n \\n \\n \\n OnlineBackup\\n \\n \\n \\n \\n NO\\n \\n \\n YES\\n \\n \\n Distribution: Churn By OnlineBackup\\n \\n\\n\",\"data:image/svg+xml;utf8,\\n\\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n 0%\\n \\n \\n 25%\\n \\n \\n 50%\\n \\n \\n 75%\\n \\n \\n 100%\\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n No\\n \\n \\n Yes\\n \\n \\n Churn\\n \\n \\n Relative Frequency\\n \\n \\n \\n DeviceProtection\\n \\n \\n \\n \\n NO\\n \\n \\n YES\\n \\n \\n Distribution: Churn By DeviceProtection\\n \\n\\n\",\"data:image/svg+xml;utf8,\\n\\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n 0%\\n \\n \\n 25%\\n \\n \\n 50%\\n \\n \\n 75%\\n \\n \\n 100%\\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n No\\n \\n \\n Yes\\n \\n \\n Churn\\n \\n \\n Relative Frequency\\n \\n \\n \\n TechSupport\\n \\n \\n \\n \\n NO\\n \\n \\n YES\\n \\n \\n Distribution: Churn By TechSupport\\n \\n\\n\",\"data:image/svg+xml;utf8,\\n\\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n 0%\\n \\n \\n 25%\\n \\n \\n 50%\\n \\n \\n 75%\\n \\n \\n 100%\\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n No\\n \\n \\n Yes\\n \\n \\n Churn\\n \\n \\n Relative Frequency\\n \\n \\n \\n StreamingTV\\n \\n \\n \\n \\n NO\\n \\n \\n YES\\n \\n \\n Distribution: Churn By StreamingTV\\n \\n\\n\",\"data:image/svg+xml;utf8,\\n\\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n 0%\\n \\n \\n 25%\\n \\n \\n 50%\\n \\n \\n 75%\\n \\n \\n 100%\\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n No\\n \\n \\n Yes\\n \\n \\n Churn\\n \\n \\n Relative Frequency\\n \\n \\n \\n StreamingMovies\\n \\n \\n \\n \\n NO\\n \\n \\n YES\\n \\n \\n Distribution: Churn By StreamingMovies\\n \\n\\n\",\"data:image/svg+xml;utf8,\\n\\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n 0%\\n \\n \\n 25%\\n \\n \\n 50%\\n \\n \\n 75%\\n \\n \\n 100%\\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n No\\n \\n \\n Yes\\n \\n \\n Churn\\n \\n \\n Relative Frequency\\n \\n \\n \\n Contract\\n \\n \\n \\n \\n \\n MONTH-TO-MONTH\\n \\n \\n ONE YEAR\\n \\n \\n TWO YEAR\\n \\n \\n Distribution: Churn By Contract\\n \\n\\n\",\"data:image/svg+xml;utf8,\\n\\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n 0%\\n \\n \\n 25%\\n \\n \\n 50%\\n \\n \\n 75%\\n \\n \\n 100%\\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n No\\n \\n \\n Yes\\n \\n \\n Churn\\n \\n \\n Relative Frequency\\n \\n \\n \\n PaperlessBilling\\n \\n \\n \\n \\n NO\\n \\n \\n YES\\n \\n \\n Distribution: Churn By PaperlessBilling\\n \\n\\n\",\"data:image/svg+xml;utf8,\\n\\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n 0%\\n \\n \\n 25%\\n \\n \\n 50%\\n \\n \\n 75%\\n \\n \\n 100%\\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n No\\n \\n \\n Yes\\n \\n \\n Churn\\n \\n \\n Relative Frequency\\n \\n \\n \\n PaymentMethod\\n \\n \\n \\n \\n \\n \\n BANK TRANSFER (AUTOMATIC)\\n \\n \\n CREDIT CARD (AUTOMATIC)\\n \\n \\n ELECTRONIC CHECK\\n \\n \\n MAILED CHECK\\n \\n \\n Distribution: Churn By PaymentMethod\\n \\n\\n\",\"data:image/svg+xml;utf8,\\n\\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n 0\\n \\n \\n 250\\n \\n \\n 500\\n \\n \\n 750\\n \\n \\n 1000\\n \\n \\n 1250\\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n 25\\n \\n \\n 50\\n \\n \\n 75\\n \\n \\n 100\\n \\n \\n MonthlyCharges\\n \\n \\n Frequency\\n \\n \\n \\n Churn\\n \\n \\n \\n \\n No\\n \\n \\n Yes\\n \\n \\n Distribution: MonthlyCharges By Churn\\n \\n\\n\",\"data:image/svg+xml;utf8,\\n\\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n 0\\n \\n \\n 500\\n \\n \\n 1000\\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n 0\\n \\n \\n 2500\\n \\n \\n 5000\\n \\n \\n 7500\\n \\n \\n TotalCharges\\n \\n \\n Frequency\\n \\n \\n \\n Churn\\n \\n \\n \\n \\n No\\n \\n \\n Yes\\n \\n \\n Distribution: TotalCharges By Churn\\n \\n\\n\",\"data:image/svg+xml;utf8,\\n\\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n 0%\\n \\n \\n 25%\\n \\n \\n 50%\\n \\n \\n 75%\\n \\n \\n \\n \\n \\n \\n \\n \\n \\n Low\\n \\n \\n Medium\\n \\n \\n PredictivePower\\n \\n \\n Relative Frequency\\n \\n \\n \\n PredictivePower\\n \\n \\n \\n \\n Low\\n \\n \\n Medium\\n \\n \\n Predictive power of features\\n \\n\\n\"],\"slickOpts\":{\"dots\":true}}],\"evals\":[],\"jsHooks\":[]} \nIt is important to keep in mind that this type of bivariate analysis cannot detect combinatorial effects that exist among multiple variables to affect churn. Therefore, just because a variable do not appear to be differently distributed in terms of churn behaviour, such as Gender, it should not be excluded from analysis as it may be significant when considered in combination with other variables. Nevertheless, this is a good start for seeing if there are “learnable” signals in the dataset.\nThe output also includes a dataframe with summary statistics pertaining to variable type, presence of outliers, and descriptive statistics.\n## Import libraries library(knitr) library(kableExtra) ## Preview data kable(t(head(autoEDA_results$overview, 4)), colnames=NULL) %\u0026gt;% kable_styling(bootstrap_options = c(\u0026quot;striped\u0026quot;, \u0026quot;hover\u0026quot;))    1  2  3  4      Feature  Churn  Contract  Dependents  DeviceProtection    Observations  7032  7032  7032  7032    FeatureClass  character  character  character  character    FeatureType  Categorical  Categorical  Categorical  Categorical    PercentageMissing  0  0  0  0    PercentageUnique  0.03  0.04  0.03  0.03    ConstantFeature  No  No  No  No    ZeroSpreadFeature  No  No  No  No    LowerOutliers  0  0  0  0    UpperOutliers  0  0  0  0    ImputationValue  NO  MONTH-TO-MONTH  NO  NO    MinValue  0  0  0  0    FirstQuartile  0  0  0  0    Median  0  0  0  0    Mean  0  0  0  0    Mode  NO  MONTH-TO-MONTH  NO  NO    ThirdQuartile  0  0  0  0    MaxValue  0  0  0  0    LowerOutlierValue  0  0  0  0    UpperOutlierValue  0  0  0  0    PredictivePowerPercentage  0  46  17  7    PredictivePower  Low  Medium  Low  Low     In the last row, there is a handy PredictivePower metric for each input variable with respect to a specified target variable. For now, we can ignore this as I will cover it in more details in a later post examining variable importance.\n ExPanDaR: your own Shiny app for data exploration ExPanDaR provides a really nifty Shiny app for interactive explorations of your data set. Originally designed for examining time-series data, the package requires the input dataframe to have a 1) time/date column and 2) a column that uniquely identifies each row. As the time/date column is only needed if you want to visualize time-dependent trends, to use a dataset without a time dimension you can just add a new numeric column (ts) with a constant and set that as the time dimension. An index column would suffice for the second requirement. In the original Telco dataset, the customerID column would have worked fine. As I had dropped it in the process of data cleaning, I will just add a new index column (ID).\n## Import library library(ExPanDaR) ## Add mock time column and new index to dataframe cleaned_df$ts \u0026lt;- rep(1, nrow(cleaned_df)) cleaned_df$ID \u0026lt;- seq.int(nrow(cleaned_df)) To start up the Shiny app for interactive exploration of the results:\nExPanD(df = cleaned_df, cs_id = \u0026quot;ID\u0026quot;, ts_id = \u0026quot;ts\u0026quot;) Here are some snapshots of the features that I find most useful. The dropdown menus and sliders make it really easy and flexible to examine any combinations of variables.\n         To go beyond bivariate relationships, the scatter plot can aggregate information from up to four variables and really give some interesting insights.\n   There are some other very cool features like allowing the user to generate and explore new variables (from some arithemtic combinations of existing variables) on the fly and performing regression analysis. Definitely give this package a try!\n     ","permalink":"/post/fast-exploratory-data-analysis-for-the-impatient/","tags":["EDA","R","Visualizations"],"title":"Fast exploratory data analysis for when you just can't wait"},{"categories":["Data science toolbox"],"contents":" \n --       \n As useful as they are, and they really are, Jupyter notebooks can feel rather stale after a few years. While they are great for quickly testing out code and exploring datasets, I can’t help but want something more fun and polished for presenting a completed project.\nFor this reason I had been working largely in R for the past while, despite my preference for the simplicity of the Python syntax, in large part due to the vibrant Shiny ecosystem that makes creating dashboards and interactive web apps easy and fun. However, I am happy to report that in really just the past year or so, the interactive app/dashboard scene in Python has really flourished, first with the appearance of the Plotly Dash platform and then most recently with Streamlit. Here is a (ever updating) round-up of my experiences so far with Python packages that allow us to bring our data science projects to life.\nStreamlit What really drew me back to Python is the appearance of Streamlit, an open-source library that really truly makes converting a data analysis workflow to an app a breeze. By adding a few magic commands, a Python script is spun to an interactive app that can be deployed on Heroku like any other web app.\n\n \nInitially, Streamlit seemed to me neither here nor there, sitting somewhere between Plotly Dash and Jupyter notebooks. While it seemed very easy to worked with, I thought that it was missing the “look” of Dash and also the versatility of cell-based operations of Jupyter. However, as soon as I gave it a try, I totally understood the allure.\nThe absolute best feature of Streamlit, in my opinion, is how easy it is to create interactive widgets like dropdown menus, radio boxes, sliders and even text/number inputs, without needing to write any callbacks. Using an example from the official documentation, this is how to create and get input from a slider:\nimport streamlit as st age = st.slider(\u0026#39;How old are you?\u0026#39;, 0, 130, 25) st.write(\u0026quot;I\u0026#39;m \u0026quot;, age, \u0026#39;years old\u0026#39;) Creating other types of interactive widgets in Streamlit is just as easy. You can find a list of functionalities currently supported here.\nIn comparison, this is how to create the same thing in Plotly Dash:\nimport dash import dash_html_components as html import dash_core_components as dcc app = dash.Dash(__name__, external_stylesheets=external_stylesheets) app.layout = html.Div([ dcc.Slider( id=\u0026#39;my-slider\u0026#39;, min=0, max=20, step=0.5, value=10, ), html.Div(id=\u0026#39;slider-output-container\u0026#39;) ]) @app.callback( dash.dependencies.Output(\u0026#39;slider-output-container\u0026#39;, \u0026#39;children\u0026#39;), [dash.dependencies.Input(\u0026#39;my-slider\u0026#39;, \u0026#39;value\u0026#39;)]) def update_output(value): return \u0026#39;You have selected \u0026quot;{}\u0026quot;\u0026#39;.format(value) if __name__ == \u0026#39;__main__\u0026#39;: app.run_server(debug=True)  Of course, Plotly Dash provides many other functionalities that Streamlit is not capable of, at least for now. However, the simplicity of working with Streamlit makes it so satistfying to quickly whip up an interactive app to showcase your work.\nFor example, I have made two Streamlit apps to host my microlearning series on survival analysis and building a random forest classifier to predict customer churn. I made them both multipage apps that allow progressive reveal of the content at the learner’s pace, in order to take advantage of the easy interactive widgets to the fullest extent. Granted that I had the workflow written out before hand, but making either one of these apps took only 2-3 days. Check out this one about integrating unsupervised and supervised machine learning!\n\n If you are interested in trying Streamlit out, there are several demo apps listed in the documentation linked above. In addition, many enthusiastic adopters of Streamlit have tweeted about their own creations.\n Plotly Dash Plotly Dash has been around for quite a while now, so I will not go as much in depth here, trusting that everyone is already pretty familiar with it. Unlike the other two packages introduced here, Dash has the benefit of the very large and active Plotly community to serve as a solid knowledge base to support users of all levels.\nAs of now, Plotly Dash just cannot be beat in terms of how polished its end products look. It is my package of choice if I need to create a dashboard/app that will be used by non-technical end users, such as business professionals, with clear interactive features and sophisticated crosstalk between elements (i.e. data tables, plots, maps, etc.). For example, here is a sales dashboard that I had made while learning the Dash platform.\n  However, as mentioned in comparison with Streamlit, the Dash code base can get quite large and complex very quickly, particularly when used with the built-in or Bootstrap grid system for layout. Consequently, it has a fairly steep learning curve, with very rewarding results. On a related note, I cannot recommend enough the Dash Bootstrap Components package, which greatly simplifies the implementations of a lot of layout and interactive features with the added benefit of the clean Bootstrap look.\nWant to get started on your own? For an step-by-step guide to building a professional dashboard, take a look at the video below made by a Plotly developer:\n \n Voilà Finally, just because we want to upgrade from Jupyter notebooks does not mean we are going to do away with it completely, as it is still a fantastic platform for exploring data and prototyping analysis workflows. In addition, the ability to use Python and R together in the same notebook makes it indispensible for data scientists who want the best of both worlds: ease of data wrangling in Python but mature analysis packages in R. Since so many of us begin a project in Jupyter notebooks, it would be a dream come true to be able to make dashboards/interactive apps from the analysis results right there. The recently released package Voilà grants that wish, somewhat.\nHere is an introduction to the package at SciPy 2019:\n\n \nWhile interactive widgets like dropdown menus and sliders can be added to the dashboard, as you can do in Streamlit and Plotly Dash, using the ipywidgets library, I have personally found the syntax much less clear and not as many tutorials/help pages available to get a newcomer started. In addition, as Voilà is still in very early stages of development, the resulting dashboard/app looks rather barebones as compared to Dash.\nHere is an “learning dashboard” that I had made using Voilà to introduce various model-agnostic approaches to calculate feature importance, for comparison with the sales dashboard made with Dash.\n  Nevertheless, as mentioned above, one strength of Jupyter notebooks is the ability to use other language kernels. Any language that is supported by a Jupyter kernel can be used to create a Voilà app, so for projects where that is needed, this would be the package for you.\n \nTaken together, it is really an exciting time for finally being able to communicate/present your Python data science projects in style. This post will be updated as new features and packages become available, so please check back once in a while! :)\n     ","permalink":"/post/making-python-apps/","tags":["Dash","Jupyter","Python","Streamlit","Voila"],"title":"Going beyond Jupyter notebooks"},{"categories":["Data science toolbox"],"contents":" \n --       \n We are always going to need Matplotlib and Seaborn, but it is so great to see new Python plotting packages popping up in the past few years. In addition to showcasing them here, we will document the little tricks and gotchas that we come across along the way.\nPlotly Express Plotly does not need much introduction, as it is now very widely used to create interactive plots in both Python and R. For a while, one major drawback of Plotly is its rather inconvenient syntax, where the source data needs to be passed in as arrays even though most of us want to be able to plot data directly from dataframes.\nWith the release of Plotly Express with its succinct Seaborn-like one-liner syntax, however, the situation is now much improved. While not all Plotly graphs can be made this way, there is an impressive variety of plots available in the Plotly Express library, including interactive maps.\nOne small annoyance is that the legends in graphs made using Plotly Express are cluttered by the column names. Following the useful tip here, you can use the .for_each_trace() function call to remove them. For example, here we will replace species= with an empty space:\nimport plotly.express as px iris = px.data.iris() fig = px.scatter(iris, x=\u0026quot;sepal_width\u0026quot;, y=\u0026quot;sepal_length\u0026quot;).for_each_trace(lambda t: t.update(name=t.name.replace(\u0026quot;species=\u0026quot;,\u0026quot;\u0026quot;)))    Here is a collection of useful links curated by us for all things Plotly:\n \nYellowbrick This package, built on top of Matplotlib, facilitates visualizing a variety of information from various stages of machine learning workflows. It is a sizable library, and looking through its API is an education in itself.\nWe have not had many chances to use Yellowbrick yet, but are looking forward to being able to delve deeper into this package in the future!\n\nPlotnine One of the strengths of the R language is its very powerful graphing package ggplot2, with its distinctive implementation of Leland Wilkinson’s Grammar of Graphics. The Plotnine package brings this (at least part of it, it seems for now) capability to Python, with very little change to the R syntax.\nThe brackets around the ggplot() function call looks strange at first, but it is needed for the signature multi-line ggplot2 grammer to work:\n## Import libraries import pandas as pd from plotnine import * from random import randint # Generate dataset random_numbers = [randint(1, 100) for p in range(0, 100)] df = pd.DataFrame({\u0026#39;number\u0026#39;: random_numbers}) # Draw plot p = ( ggplot(df, aes(x=\u0026#39;number\u0026#39;)) + geom_histogram(bins=20, na_rm=True) + ggtitle(\u0026#39;Histogram of random numbers\u0026#39;) + theme_light() ) ## Display plot p.draw(); If you want to save the plot to file:\np.save(\u0026quot;output.png\u0026quot;) And just because I can see myself wanting to use these plots in a Streamlit app, here is a working template:\nimport pandas as pd import numpy as np from plotnine import * import streamlit as st n = 10 df = pd.DataFrame({\u0026#39;x\u0026#39;: np.arange(n), \u0026#39;y\u0026#39;: np.arange(n), \u0026#39;yfit\u0026#39;: np.arange(n) + np.tile([-.2, .2], n // 2), \u0026#39;cat\u0026#39;: [\u0026#39;a\u0026#39;, \u0026#39;b\u0026#39;] * (n // 2)}) a = ( ggplot(df) + geom_col(aes(\u0026#39;x\u0026#39;, \u0026#39;y\u0026#39;)) ) fig = a.draw(); ## Needed to remove the \u0026quot;ggplot\u0026lt;#\u0026gt;\u0026quot; message st.pyplot() Finally, as always, here is a (ever growing) collection of links curated by us to get your started with using Plotnine:\n     ","permalink":"/post/graphing-in-python-a-walkthrough/","tags":["Python","Visualizations"],"title":"Graphing in Python - New(er) kids on the block"},{"categories":["Data science toolbox"],"contents":"       \n --       \n Business applications of data science is obviously a very broad topic, as data-driven approaches are becoming increasingly integrated into corporate practices. For this reason, this collection will begin as a scaffold of the topics that we are currently using or want to become familiar with in the near future. Be sure to check back regularly as we add add more content!\nGetting data    .accordion { background-color: #FCB97D; color: #F2CC8F; cursor: pointer; padding: 14px; width: 100%; border: none; text-align: left; outline: none; font-size: 13px; transition: 0.4s; } .active, .accordion:hover { background-color: #F2CC8F; } .panel { padding: 0 18px; background-color: white; max-height: 0; overflow: hidden; transition: max-height 0.2s ease-out; } h3 { padding-top: 0; font-size: 15px; } .content h3 { margin-bottom: 0; }     Working with Excel files    Read data from Excel file:\nimport pandas as pd df = pd.read_excel(\u0026#39;https://archive.ics.uci.edu/ml/machine-learning-databases/00352/Online%20Retail.xlsx\u0026#39;)  {\"x\":{\"tag\":{\"name\":\"Reactable\",\"attribs\":{\"data\":{\"InvoiceNo\":[[536365],[536365],[536365]],\"StockCode\":[[\"85123A\"],[71053],[\"84406B\"]],\"Description\":[[\"WHITE HANGING HEART T-LIGHT HOLDER\"],[\"WHITE METAL LANTERN\"],[\"CREAM CUPID HEARTS COAT HANGER\"]],\"Quantity\":[6,6,8],\"InvoiceDate\":[\"2010-12-01T08:26:00\",\"2010-12-01T08:26:00\",\"2010-12-01T08:26:00\"],\"UnitPrice\":[2.55,3.39,2.75],\"CustomerID\":[17850,17850,17850],\"Country\":[\"United Kingdom\",\"United Kingdom\",\"United Kingdom\"]},\"columns\":[{\"accessor\":\"InvoiceNo\",\"name\":\"InvoiceNo\",\"type\":\"list\"},{\"accessor\":\"StockCode\",\"name\":\"StockCode\",\"type\":\"list\"},{\"accessor\":\"Description\",\"name\":\"Description\",\"type\":\"list\"},{\"accessor\":\"Quantity\",\"name\":\"Quantity\",\"type\":\"numeric\"},{\"accessor\":\"InvoiceDate\",\"name\":\"InvoiceDate\",\"type\":\"Date\"},{\"accessor\":\"UnitPrice\",\"name\":\"UnitPrice\",\"type\":\"numeric\"},{\"accessor\":\"CustomerID\",\"name\":\"CustomerID\",\"type\":\"numeric\"},{\"accessor\":\"Country\",\"name\":\"Country\",\"type\":\"character\"}],\"defaultPageSize\":10,\"paginationType\":\"numbers\",\"showPageInfo\":true,\"minRows\":1,\"dataKey\":\"e3d7eb06698ef9fb96ced2163802ea0a\"},\"children\":[]},\"class\":\"reactR_markup\"},\"evals\":[],\"jsHooks\":[]} \n  Optical Character Recognition (OCR) of documents    Often, we need to extract data from print or scanned business documents, which is where these packages can come in handy:\n \n  Text parsing using regular expressions    To process OCR results into clean tabular data:\n   Data wrangling Given the topic, here we will focus on replicating common Excel functionalities/tasks in Python and R:\n Pivot tables    More coming soon! \n  Crosstab    More coming soon! \n  Sets    More coming soon! \n  Vlookup    More coming soon! \n  Analysis techniques This will be a collection of more “traditional” business analytics approaches. For machine learning methods, please see a future post devoted to the topic.\n Association rule mining    More coming soon! \n  RFM analysis    More coming soon! \n  Survival analysis    More coming soon! \n  Time-series analysis    More coming soon! \n  Reporting Good results are only useful when they are effectively communicated:\n Dashboards    More coming soon! \n  Presentation slides    More coming soon! \n  Automated reports    More coming soon! \n  var acc = document.getElementsByClassName(\"accordion\"); var i; for (i = 0; i        ","permalink":"/post/data-science-for-business/","tags":["Association rule mining","Python","R","Survival analysis"],"title":"Data science for business"},{"categories":["Dev handbook"],"contents":" \n --       \n  Where do we start with version control? It’s a bit finicky, often neglected, but oh-so important.\nOne of the key components of building good habits is to make the action easy to do. So here we round up the (usual) cheat sheets and some fun tools that make learning/remembering those git commands a little easier.\nCheat sheet Of course, to start us off, the official cheat sheet from Github:\n \nGit Explorer As promised, here is a fun one:\n     ","permalink":"/post/version-control-git-handbook/","tags":["Git"],"title":"Version control with Git"},{"categories":["Web development"],"contents":" \n       \n Making your own website or app is a great creative outlet. This is a continuously updated repository of free (or reasonably priced) resources for spicing up the UI/UX:\nIllustrations For those of us who are utterly not artistically inclined, jazz up landing pages and blog posts with these fantastic free illustration. Many of the images you see on this site are taken from these sites:\n \nIcons \u0026amp; Glyphs Pepper these through out to keep things interesting:\n \nColour schemes For those of us who know what looks good but can’t come up with it on our own, colour palette generators and inspirations are essential:\n     ","permalink":"/post/ui-and-ux-resources/","tags":["UI","UX"],"title":"Handy UI and UX resources "},{"categories":["Careers"],"contents":" \n --       \n There may exist some people in this world who enjoy job hunting, but we have not met one of them yet. As data scientist/developer jobs are becoming more and more popular, it is all the more important to know how to keep track of the most interesting opportunity and make your application to stand out. Here are a continuously updating suite of tools and services that we have tried or want to try for getting that next job.\nResumes One way to get yourself to stop putting off writing or updating your resume is make doing it fun and attractive. Remember the good old days of fiddling with Microsoft Word margins to get that two-column look just right? In recent years, almost too many online resume template/building platforms have popped up to keep track of. Here we have a collection of such platforms that offer at least some level of free services.\nFor creating the “official” resume to be send out, our favourites are CakeResume and Flow CV, as they offer the most innovative interfaces, generous free tiers and just the right amount of flexibility. For something a bit different, Qwilir offers templates to create a single-page web document to display the most salient points of your resume, which can be linked as part of a professional portfolio. Nevertheless, as everyone’s resume needs and aesthetic sensibilities differ, it might be worthwhile to look through each of the links below to find one that you particularly like.\n\n \nFinally, we have not tried it yet, but Resume Worded offers a free AI-powered review of your data science resume to check for important key words.\n    ","permalink":"/post/job-hunting-tools/","tags":["Job search","Resume","Useful"],"title":"Make job hunting fun again (kind of)"},{"categories":["Web development"],"contents":" \n --      h3 {font-size: 24px;}    \n Having made a few websites/web apps using Django, Blogdown and Shiny, some HTML and CSS snippets have turned out to be useful time and time again. Not being professional front-end developers, we thought it would probably be a good idea to create an easy-to-reference repository of them for future projects.\nNavigation Tabs Tabs are a great way to display information that you might want to compare side-by-side. The horizontal layout also helps to save on space. I always opt for the Bootstrap tabsets, as they are very easy and consistent to implement:\n\nTab 1  Tab 2  Tab 3     Image credit: Icons 8       Image credit: Icons 8       Image credit: Icons 8       \nTo make this:\n\u0026lt;div class=\u0026quot;container\u0026quot;\u0026gt; \u0026lt;!-- Nav tabs --\u0026gt; \u0026lt;ul class=\u0026quot;nav nav-pills nav-justified\u0026quot; role=\u0026quot;tablist\u0026quot;\u0026gt; \u0026lt;li class=\u0026quot;nav-item active\u0026quot;\u0026gt; \u0026lt;a class=\u0026quot;nav-link active\u0026quot; data-toggle=\u0026quot;tab\u0026quot; href=\u0026quot;#tab1\u0026quot;\u0026gt;\u0026lt;font size=\u0026quot;+2\u0026quot;\u0026gt;\u0026lt;b\u0026gt;Tab 1\u0026lt;/b\u0026gt;\u0026lt;/font\u0026gt;\u0026lt;/a\u0026gt; \u0026lt;/li\u0026gt; \u0026lt;li class=\u0026quot;nav-item\u0026quot;\u0026gt; \u0026lt;a class=\u0026quot;nav-link\u0026quot; data-toggle=\u0026quot;tab\u0026quot; href=\u0026quot;#tab2\u0026quot;\u0026gt;\u0026lt;font size=\u0026quot;+2\u0026quot;\u0026gt;\u0026lt;b\u0026gt;Tab 2\u0026lt;/b\u0026gt;\u0026lt;/font\u0026gt;\u0026lt;/a\u0026gt; \u0026lt;/li\u0026gt; \u0026lt;li class=\u0026quot;nav-item\u0026quot;\u0026gt; \u0026lt;a class=\u0026quot;nav-link\u0026quot; data-toggle=\u0026quot;tab\u0026quot; href=\u0026quot;#tab3\u0026quot;\u0026gt;\u0026lt;font size=\u0026quot;+2\u0026quot;\u0026gt;\u0026lt;b\u0026gt;Tab 3\u0026lt;/b\u0026gt;\u0026lt;/font\u0026gt;\u0026lt;/a\u0026gt; \u0026lt;/li\u0026gt; \u0026lt;/ul\u0026gt; \u0026lt;!-- Tab panes --\u0026gt; \u0026lt;div class=\u0026quot;tab-content\u0026quot;\u0026gt; \u0026lt;div id=\u0026quot;tab1\u0026quot; class=\u0026quot;container tab-pane active\u0026quot;\u0026gt; Tab 1 content \u0026lt;/div\u0026gt; \u0026lt;div id=\u0026quot;tab2\u0026quot; class=\u0026quot;container tab-pane fade\u0026quot;\u0026gt; Tab 2 content \u0026lt;/div\u0026gt; \u0026lt;div id=\u0026quot;tab3\u0026quot; class=\u0026quot;container tab-pane fade\u0026quot;\u0026gt; Tab 3 content \u0026lt;/div\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;/div\u0026gt; \nFloating outline When the page has a lot of content, it is very helpful to include a floating table of contents on the side of the page that shows the reader where they are. You can see an example of this on the left!\nAfter experimenting with a few options, I found that the steps outlined here by Aidan Feldman work most easily and the best for Blogdown sites:\n\u0026lt;head\u0026gt; \u0026lt;meta charset=\u0026quot;utf-8\u0026quot;\u0026gt; \u0026lt;meta name=\u0026quot;viewport\u0026quot; content=\u0026quot;width=device-width, initial-scale=1\u0026quot;\u0026gt; \u0026lt;link rel=\u0026quot;stylesheet\u0026quot; href=\u0026quot;https://cdn.rawgit.com/afeld/bootstrap-toc/v1.0.1/dist/bootstrap-toc.min.css\u0026quot; /\u0026gt; \u0026lt;script src=\u0026quot;https://ajax.googleapis.com/ajax/libs/jquery/3.4.1/jquery.min.js\u0026quot;\u0026gt;\u0026lt;/script\u0026gt; \u0026lt;script src=\u0026quot;https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.7/umd/popper.min.js\u0026quot;\u0026gt;\u0026lt;/script\u0026gt; \u0026lt;script src=\u0026quot;https://maxcdn.bootstrapcdn.com/bootstrap/4.3.1/js/bootstrap.min.js\u0026quot;\u0026gt;\u0026lt;/script\u0026gt; \u0026lt;script src=\u0026quot;https://cdn.rawgit.com/afeld/bootstrap-toc/v1.0.1/dist/bootstrap-toc.min.js\u0026quot;\u0026gt;\u0026lt;/script\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;body data-spy=\u0026quot;scroll\u0026quot; data-target=\u0026quot;#toc\u0026quot;\u0026gt; \u0026lt;div class=\u0026quot;container\u0026quot;\u0026gt; \u0026lt;div class=\u0026quot;row\u0026quot;\u0026gt; \u0026lt;!-- sidebar, which will move to the top on a small screen --\u0026gt; \u0026lt;div class=\u0026quot;col-sm-2\u0026quot;\u0026gt; \u0026lt;nav id=\u0026quot;toc\u0026quot; data-toggle=\u0026quot;toc\u0026quot; class=\u0026quot;sticky-top\u0026quot; style=\u0026#39;padding-top:40px\u0026#39;\u0026gt;\u0026lt;/nav\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;!-- main content area --\u0026gt; \u0026lt;div class=\u0026quot;col-sm-10\u0026quot;\u0026gt; \u0026lt;!-- Page content --\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;/body\u0026gt; \nIf there are headings that you do not want to be included in the outline, you can change the header HTML tag, like \u0026lt;h2\u0026gt;, to \u0026lt;h2 data-toc-skip\u0026gt;.\n\nStyling Font size The font size of specific sections of text can be altered, either by relative change:\n\u0026lt;font size=\u0026quot;+2\u0026quot;\u0026gt;This is bigger text.\u0026lt;/font\u0026gt; Or by setting the absolute size (1-7):\n\u0026lt;font size=\u0026quot;1\u0026quot;\u0026gt;This is really tiny text.\u0026lt;/font\u0026gt; \nCode blocks We love the ability to use both Python and R code in a single Rmarkdown post on Blogdown sites. With that, it would be nice to be able to easily visually distinguish Python and R code blocks. Here are some CSS stylings used on this site:\ncode{ /* Base styling for all code blocks */ padding: 3px 5px; background: #ffffff; border: 1px solid $border-color; border-radius: 3px; color: $text-color-dark; } .python { background: #ffffff; border-color: #F6B156; border-style: dotted; page-break-inside: avoid; font-family: monospace; font-size: 15px; line-height: 1.6; margin-bottom: 1.6em; max-width: 100%; overflow: auto; padding: 1em 1.5em; display: block; word-wrap: break-word; } .r { background: #ffffff; border-color: #03d944; border-style: dotted; page-break-inside: avoid; font-family: monospace; font-size: 15px; line-height: 1.6; margin-bottom: 1.6em; max-width: 100%; overflow: auto; padding: 1em 1.5em; display: block; word-wrap: break-word; }     ","permalink":"/post/neat-html-snippets-to-jazz-up-your-site/","tags":["Useful"],"title":"HTML and CSS snippets to jazz up your site"},{"categories":null,"contents":"Welcome to Intelligence Refinery!\nWe are Nancy and Mihai Chelaru-Centea, two neuroscience majors who not so long ago ditched the lab notebooks for Jupyter notebooks. With the explosion of the data science field, there is a bewildering number of articles, tutorials, Stackoverflow answers, courses and packages on the internet for anyone who cares to look. Like many other newcomers, we are navigating this ever rising sea of information, and often misinformation, with varying degrees of success each day.\nAfter having worked for some time as data scientist/developers and meeting others on the same path, we realized the importance of creating a dedicated repository in which we can continuously refine and grow our professional knowledge, so as to be able to keep pace with this fast moving field. So, we rolled up our sleeves and built this site, where we try to distill everything we are learning about data science, software development, and anything else of interest into helpful nuggets for ourselves and whoever else that may stumble onto this site.\nWe hope that you enjoy your time here, and would love to hear any comments or questions that you have!\n","permalink":"/about/about/","tags":null,"title":"About Us"}]
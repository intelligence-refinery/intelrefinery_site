[{"categories":["Data science"],"contents":" A second edition, *Practical Statistics for Data Scientists: 50+ Essential Concepts Using R and Python* is scheduled for publication on June 9, 2020.  \nWho is this book for? According to the preface:\nThis book is aimed at the data scientist with some familiarity with the R programming language, and with some prior (perhaps spotty or ephemeral) exposure to statistics.  Two goals underlie this book: 1) to lay out, in digestible, navigable, and easily referenced form, key concepts from statistics that are relevant to data science, and 2) to explain which concepts are important and useful from a data science perspective, which are less so, and why.  \nBook outline  1. Exploratory Data Analysis  Elements of Structured Data Rectangular Data Estimates of Location Estimates of Variability Exploring the Data Distribution Exploring Binary and Categorical Data Correlation Exploring Two or More Variables Summary    2. Data and Sampling Distributions  Random Sampling and Sample Bias Selection Bias Sampling Distribution of a Statistic The Bootstrap Confidence Intervals Normal Distribution Long-Tailed Distributions Student\u0026rsquo;s t-Distribution Binomial Distribution Poisson and Related Distributions Summary    3. Statistical Experiments and Significance Testing  A/B Testing Hypothesis Tests Resampling Statistical Significance and P-values t-Tests Multiple Testing Degrees of Freedom ANOVA Chi-Square Test Multi-Arm Bandit Algorithm Power and Sample Size Summary    4. Regression and Prediction  Simple Linear Regression Multiple Linear Regression Prediction Using Regression Factor Variables in Regression Interpreting the Regression Equation Testing the Assumptions: Regression Diagnostics Polynomial and Spline Regression Summary    5. Classification  Naive Bayes Discriminant Analysis Logistic Regression Evaluating Classification Models Strategies for Imbalanced Data Summary    6. Statistical Machine Learning  K-Nearest Neighbours Tree Models Bagging and the Random Forest Boosting    7. Unsupervised Learning  Principal Components Analysis K-Means Clustering Hierarchical Clustering Model-Based Clustering Scaling and Categorical Variables Summary  \n\nImpression Strengths  Book is divided up into concepts, which provides useful chunks to guide learning\n Weaknesses  The amount of equations included in the text can interrupt flow and be confusing to readers without a significant statistics background\n \n","permalink":"/post/practical-statistics-for-data-scientists/","tags":["Textbook"],"title":"Practical Statistics for Data Scientists"},{"categories":["Natural language processing"],"contents":"         \n textrank textrank is a R package that uses the textrank algorithm to 1) rank sentences by their “importance” and 2) identify keywords that appear more frequently in the test. The workflow presented here is a combination of posts by Jan Wijffels and Emil Hvitfeldt.\nWorflow Get text Let’s start by grabbing the article text used in the post by Emil Hvitfeldt, which is about a newly released Fitbit for kids. Here, I’m using a slightly different rvest command to scrape the article, as the instructions provided scraped a lot of extra website boilerplate text that muddied the waters, so to speak, and screwed up the textrank results.\n## Import libraries library(tidyverse) library(tidytext) library(textrank) library(rvest) url \u0026lt;- \u0026quot;http://time.com/5196761/fitbit-ace-kids-fitness-tracker/\u0026quot; article \u0026lt;- read_html(url) %\u0026gt;% html_nodes(\u0026#39;p\u0026#39;) %\u0026gt;% html_text() article \u0026lt;- paste(article, collapse = \u0026quot;\\n\u0026quot;) print(article) ## [1] \u0026quot;Fitbit is launching a new fitness tracker designed for children called the Fitbit Ace, which will go on sale for $99.95 in the second quarter of this year.\\nThe Fitbit Ace looks a lot like the company’s Alta tracker, but with a few child-friendly tweaks. The most important of which is Fitbit’s new family account option, which gives parents control over how their child uses their tracker and is compliant with the Children’s Online Privacy Protection Act, or COPPA. Parents must approve who their child can connect with via the Fitbit app and can view their kid’s activity progress and sleep trends, the latter of which can help them manage their children’s bedtimes.\\nLike many of Fitbit’s other products, the Fitbit Ace can automatically track steps, monitor active minutes, and remind kids to move when they’ve been still for too long. But while Fitbit’s default move goal is 30 minutes for adult users, the Ace’s will be 60 minutes, in line with the World Health Organization’s recommendation that children between the ages of five and 17 get an hour of daily physical activity per day. Fitbit says the tracker is designed for children eight years old and up.\\nFitbit will also be introducing a Family Faceoff feature that lets kids compete in a five-day step challenge against the other members of their family account. The app also will reward children with in-app badges for achieving their health goals. Fitbit’s new child-friendly fitness band will be available in blue and purple, is showerproof, and should last for five days on a single charge.\\nThe Ace launch is part of Fitbit’s broader goal of branching out to new audiences. The company also announced a new smartwatch on Tuesday called the Versa, which is being positioned as an everyday smartwatch rather than a fitness-only device or sports watch, like some of the company’s other products.\\nAbove all else, the Ace is an effort to get children up and moving. The Centers for Disease Control and Prevention report that the percentage of children and adolescents affected by obesity has more than tripled since the 1970’s. But parents who want to encourage their children to move already have several less expensive options to choose from. Garmin’s $79.99 Vivofit Jr. 2, for example, comes in themed skins like these Minnie Mouse and Star Wars versions, while the wristband entices kids to move by reflecting their fitness achievements in an accompanying smartphone game. The $39.99 Nabi Compete, meanwhile, is sold in pairs so that family members can work together to achieve movement milestones.\\nContact us at editors@time.com.\u0026quot;  Part-of-speech tagging As the textrank algorithm measures similiarity between sentences by the extend of word overlap between them, it is important to compare them only in terms of the most informative words. This is done by part-of-speech (POS) tagging, to identify nouns, verbs and adjectives.\nHere we will use the udpipe package (Github repo), which provides language-agnostic tokenization, tagging, lemmatization and dependency parsing of raw text.\n## Import library library(udpipe) ## Create model tagger \u0026lt;- udpipe_download_model(\u0026quot;english\u0026quot;) tagger \u0026lt;- udpipe_load_model(tagger$file_model) ## Annotate text article \u0026lt;- udpipe_annotate(tagger, article) Let’s look at the results:\narticle \u0026lt;- as.data.frame(article) article %\u0026gt;% head(10) %\u0026gt;% select(-sentence) %\u0026gt;% kable() %\u0026gt;% kable_styling(bootstrap_options = c(\u0026quot;striped\u0026quot;, \u0026quot;hover\u0026quot;), full_width = F) %\u0026gt;% scroll_box(width = \u0026quot;100%\u0026quot;)   doc_id  paragraph_id  sentence_id  token_id  token  lemma  upos  xpos  feats  head_token_id  dep_rel  deps  misc      doc1  1  1  1  Fitbit  Fitbit  PROPN  NNP  Number=Sing  3  nsubj  NA  NA    doc1  1  1  2  is  be  AUX  VBZ  Mood=Ind|Number=Sing|Person=3|Tense=Pres|VerbForm=Fin  3  aux  NA  NA    doc1  1  1  3  launching  launch  VERB  VBG  Tense=Pres|VerbForm=Part  0  root  NA  NA    doc1  1  1  4  a  a  DET  DT  Definite=Ind|PronType=Art  7  det  NA  NA    doc1  1  1  5  new  new  ADJ  JJ  Degree=Pos  7  amod  NA  NA    doc1  1  1  6  fitness  fitness  NOUN  NN  Number=Sing  7  compound  NA  NA    doc1  1  1  7  tracker  tracker  NOUN  NN  Number=Sing  3  obj  NA  NA    doc1  1  1  8  designed  design  VERB  VBN  Tense=Past|VerbForm=Part  7  acl  NA  NA    doc1  1  1  9  for  for  ADP  IN  NA  10  case  NA  NA    doc1  1  1  10  children  child  NOUN  NNS  Number=Plur  8  obl  NA  NA       Extract keywords  “In order to find relevant keywords, the textrank algorithm constructs a word network. This network is constructed by looking which words follow one another. A link is set up between two words if they follow one another, the link gets a higher weight if these 2 words occur more frequenctly next to each other in the text.”\n The only input to textrank_keywords() is the lemmatized words, with the option of filtering for only certain types of words, such as nouns, verbs and adjectives:\narticle_keywords \u0026lt;- textrank_keywords(article$lemma, relevant = article$upos %in% c(\u0026quot;NOUN\u0026quot;, \u0026quot;VERB\u0026quot;, \u0026quot;ADJ\u0026quot;)) article_keywords$keywords %\u0026gt;% subset(freq \u0026gt; 1) %\u0026gt;% kable()    keyword  ngram  freq      child  1  12    tracker  1  4    kid  1  4    move  1  4    parent  1  3    app  1  3    other  1  3    goal  1  3    fitness  1  3    year  1  2    family-account  2  2    account  1  2    activity  1  2    step  1  2    family  1  2    new  1  2      Rank sentences Loosely, the textrank algorithm computes similarities between a pair of sentences by the number of words that they have in common.\nThe textrank_sentences() function takes in two inputs 1:\nA dataframe with sentences:\nsentences \u0026lt;- unique(article[, c(\u0026quot;sentence_id\u0026quot;, \u0026quot;sentence\u0026quot;)]) A dataframe with words which are part of each sentence:\nterminology \u0026lt;- article %\u0026gt;% filter(upos %in% c(\u0026quot;NOUN\u0026quot;, \u0026quot;VERB\u0026quot;, \u0026quot;ADJ\u0026quot;)) %\u0026gt;% select(sentence_id, lemma)  Based on these two dataframes, the function calculates the pairwise distance between each sentence by computing how many terms are overlapping, in terms of Jaccard distance. These pairwise distances among the sentences are next passed on to Google’s pagerank algorithm to identify the most relevant sentences.\ntr \u0026lt;- textrank_sentences(data = sentences, terminology = terminology) Let’s see how the sentences rank:\nkable(tr[[\u0026#39;sentences\u0026#39;]] %\u0026gt;% arrange(-textrank))    textrank_id  sentence  textrank      1  Fitbit is launching a new fitness tracker designed for children called the Fitbit Ace, which will go on sale for $99.95 in the second quarter of this year.  0.0784357    15  Above all else, the Ace is an effort to get children up and moving.  0.0672520    4  The most important of which is Fitbit’s new family account option, which gives parents control over how their child uses their tracker and is compliant with the Children’s Online Privacy Protection Act, or COPPA.  0.0652963    12  Fitbit’s new child-friendly fitness band will be available in blue and purple, is showerproof, and should last for five days on a single charge.  0.0626908    8  But while Fitbit’s default move goal is 30 minutes for adult users, the Ace’s will be 60 minutes, in line with the World Health Organization’s recommendation that children between the ages of five and 17 get an hour of daily physical activity per day.  0.0621032    9  Fitbit says the tracker is designed for children eight years old and up.  0.0615568    17  But parents who want to encourage their children to move already have several less expensive options to choose from.  0.0591652    11  The app also will reward children with in-app badges for achieving their health goals.  0.0588857    5  Parents must approve who their child can connect with via the Fitbit app and can view their kid’s activity progress and sleep trends, the latter of which can help them manage their children’s bedtimes.  0.0578003    3  Ace looks a lot like the company’s Alta tracker, but with a few child-friendly tweaks.  0.0572568    10  Fitbit will also be introducing a Family Faceoff feature that lets kids compete in a five-day step challenge against the other members of their family account.  0.0561673    14  The company also announced a new smartwatch on Tuesday called the Versa, which is being positioned as an everyday smartwatch rather than a fitness-only device or sports watch, like some of the company’s other products.  0.0498193    6  Like many of Fitbit’s other products, the Fitbit  0.0454276    16  The Centers for Disease Control and Prevention report that the percentage of children and adolescents affected by obesity has more than tripled since the 1970’s.  0.0387110    7  Ace can automatically track steps, monitor active minutes, and remind kids to move when they’ve been still for too long.  0.0370217    19  Wars versions, while the wristband entices kids to move by reflecting their fitness achievements in an accompanying smartphone game.  0.0351309    2  The Fitbit  0.0332401    13  The Ace launch is part of Fitbit’s broader goal of branching out to new audiences.  0.0299872    21  Nabi Compete, meanwhile, is sold in pairs so that family members can work together to achieve movement milestones.  0.0209157    18  Garmin’s $79.99 Vivofit Jr. 2, for example, comes in themed skins like these Minnie Mouse and Star  0.0077121    20  The $39.99  0.0077121    22  Contact us at editors@time.com.  0.0077121     Makes sense, but it would be nice to have a human-made summary for comparison.\nAlternatively, we can see what the top five most “informative” sentences are in their order of appearance:\ns \u0026lt;- summary(tr, n = 5, keep.sentence.order = TRUE) cat(s, sep = \u0026quot;\\n\\n\u0026quot;) ## Fitbit is launching a new fitness tracker designed for children called the Fitbit Ace, which will go on sale for $99.95 in the second quarter of this year. ## ## The most important of which is Fitbit’s new family account option, which gives parents control over how their child uses their tracker and is compliant with the Children’s Online Privacy Protection Act, or COPPA. ## ## But while Fitbit’s default move goal is 30 minutes for adult users, the Ace’s will be 60 minutes, in line with the World Health Organization’s recommendation that children between the ages of five and 17 get an hour of daily physical activity per day. ## ## Fitbit’s new child-friendly fitness band will be available in blue and purple, is showerproof, and should last for five days on a single charge. ## ## Above all else, the Ace is an effort to get children up and moving. Finally, we can also visualize the textrank value of sentences by their order of appearance in the text.\ntr[[\u0026quot;sentences\u0026quot;]] %\u0026gt;% ggplot(aes(textrank_id, textrank, fill = textrank_id)) + geom_col(color=\u0026quot;blue\u0026quot;, fill=rgb(0.1,0.4,0.5,0.7)) + theme_classic() + guides(fill = \u0026quot;none\u0026quot;) + labs(x = \u0026quot;Sentence\u0026quot;, y = \u0026quot;TextRank score\u0026quot;)   Another example Lately, I’ve been trying to analyze job descriptions. Let’s see how textrank summarization performs on one for a data scientist position.\nlink \u0026lt;- \u0026quot;https://ca.indeed.com/viewjob?cmp=Slice-Insurance-Technologies-Inc.\u0026amp;t=Data+Scientist\u0026amp;jk=35c9914ee9b0a052\u0026amp;sjdu=vQIlM60yK_PwYat7ToXhk3myaatk1OsAkhQIw2UEit1eiAfw8fAJl6BKuZ5V5P0TgQjo7h2qNGvAZ_80Lr3XxA\u0026amp;tk=1e1nuvuml0np3000\u0026amp;adid=335410488\u0026amp;pub=4a1b367933fd867b19b072952f68dceb\u0026amp;vjs=3\u0026quot; description \u0026lt;- tryCatch( as.character(link) %\u0026gt;% read_html() %\u0026gt;% html_nodes(xpath=\u0026#39;//*[(@id = \u0026quot;jobDescriptionText\u0026quot;)]\u0026#39;) %\u0026gt;% map(xml_contents), error=function(e){NA} ) if (is.null(description)){ desc \u0026lt;- NA } final \u0026lt;- tryCatch( paste(as.character(description[[1]]), collapse=\u0026#39; \u0026#39;), error=function(e){NA} ) if (is.null(description)){ NA } job_des_raw \u0026lt;- html_text(read_html(final)) print(job_des_raw) ## [1] \u0026quot;WE are Slice Labs (Slice). We’re building a team of innovators and disruptors to change the insurance experience to meet the demands of an on-demand world. We are disrupting the idea that insurance needs to be a fixed product, with a fixed term, with fixed coverage. We believe it can and should be all digital and on-demand, so customers get only the coverage they need, right when they need it. At Slice, we’re focused on ensuring our products provide a positive, individualized customer experience. Our smaller teams invite broader thinking and problem solving, where nobody is pigeonholed into a predetermined role. We work in an open, supportive, environment that values and promotes inclusiveness, innovation, and collaboration. It’s fast-paced, dynamic and fulfilling.\\n *The Data Science team is key to Slice’s ongoing success. The data we have and the models we build are foundational to our platform and quite literally drive our business. Our work improves the customer experience, grows our market share and drives business outcomes. This is an opportunity to play an active part in delivering our data vision and determining how we get there. All that to say: Data Science work here at Slice is far from theoretical.\\n This ROLE develops and executes Data Science projects across the company. This means hands-on work at all points in the data lifecycle, including data wrangling and mining; feature engineering; model-building and testing; and implementation and communication. Your work will ensure that all our decisions are data-driven - this means that you will have a direct impact on the customer experience by influencing critical decisions on resource deployment and customer engagement. Our immediate applications include marketing analytics, fraud prevention, and risk/value modeling. (Our future applications are boundless!)\\n YOU are passionate about data. You enjoy being able to combine your analytical, technical and business skills in one role. You love solving problems and predicting behaviours. You are a collaborator and a communicator and are energized by working with multidisciplinary teams. You are a hands-on learner and are excited by the thought of moving past theory and examples to real-time data science work.\\n You bring: \\n An effective communication style with an ability to translate “the complex” to “the simple”. You are adept at data visualizations and have experience working with real-time data.4+ years of practical experience using data, models, and common sense to solve tough problems in a collaborative environment. Experience deploying models is a plus!Working experience with Python, including Pandas, NumPy, scikit-learn, NLTK, and Keras/TensorFlow.An understanding of statistical and predictive modeling concepts, machine learning algorithms, clustering and classification techniques.Some exposure to one or more sub-fields of data science, especially GIS/spatial analysis; graph theory/network analysis; or natural language processing (NLP).Working experience with non-Excel BI tools such as Tableau, Looker, Superset, PowerBI, etc.You are comfortable building datasets using traditional relational databases and you’re familiar with alternative databases (noSQL, graph databases) or big data platforms such as Apache Spark.University degree in engineering, applied statistics, data mining, machine learning, mathematics or a related quantitative discipline.*\\n Job Types: Full-time, Permanent\\n Experience:\\n Data Science: 4 years (Required)Location:\\n Ottawa, ON (Preferred)\u0026quot; Part-of-speech tagging:\n## Create model tagger \u0026lt;- udpipe_download_model(\u0026quot;english\u0026quot;) tagger \u0026lt;- udpipe_load_model(tagger$file_model) job_des \u0026lt;- udpipe_annotate(tagger, job_des_raw) job_des \u0026lt;- as.data.frame(job_des) job_des %\u0026gt;% head(10) %\u0026gt;% select(-sentence) %\u0026gt;% kable() %\u0026gt;% kable_styling(bootstrap_options = c(\u0026quot;striped\u0026quot;, \u0026quot;hover\u0026quot;), full_width = F) %\u0026gt;% scroll_box(width = \u0026quot;100%\u0026quot;)   doc_id  paragraph_id  sentence_id  token_id  token  lemma  upos  xpos  feats  head_token_id  dep_rel  deps  misc      doc1  1  1  1  WE  we  PRON  PRP  Case=Nom|Number=Plur|Person=1|PronType=Prs  4  nsubj  NA  NA    doc1  1  1  2  are  be  AUX  VBP  Mood=Ind|Tense=Pres|VerbForm=Fin  4  cop  NA  NA    doc1  1  1  3  Slice  slice  PROPN  NNP  Number=Sing  4  compound  NA  NA    doc1  1  1  4  Labs  labs  PROPN  NNP  Number=Sing  0  root  NA  NA    doc1  1  1  5  (  (  PUNCT  -LRB-  NA  6  punct  NA  SpaceAfter=No    doc1  1  1  6  Slice  slice  PROPN  NNP  Number=Sing  4  appos  NA  SpaceAfter=No    doc1  1  1  7  )  )  PUNCT  -RRB-  NA  6  punct  NA  SpaceAfter=No    doc1  1  1  8  .  .  PUNCT  .  NA  4  punct  NA  NA    doc1  1  2  1  We  we  PRON  PRP  Case=Nom|Number=Plur|Person=1|PronType=Prs  2  nsubj  NA  SpaceAfter=No    doc1  1  2  2  ’re  ’re  VERB  VBP  Mood=Ind|Tense=Pres|VerbForm=Fin  0  root  NA  NA      The keyword extraction functionality seemed lack-luster for this example too, giving what is essentially a list of most frequently appearing words.\njob_des_keywords \u0026lt;- textrank_keywords(job_des$lemma, relevant = job_des$upos %in% c(\u0026quot;NOUN\u0026quot;, \u0026quot;VERB\u0026quot;, \u0026quot;ADJ\u0026quot;)) job_des_keywords$keywords %\u0026gt;% subset(freq \u0026gt; 1) %\u0026gt;% kable()    keyword  ngram  freq      data  1  12    experience  1  8    work  1  8    ’re  1  3    team  1  3    demand  1  3    customer-experience  2  3    problem  1  3    role  1  3    business  1  3    data-science  2  3    science  1  3    learn  1  3    database  1  3    product  1  2    fix  1  2    customer  1  2    mean  1  2    building  1  2    communication  1  2    modeling  1  2    theory  1  2    work-experience  2  2    analysis  1  2    such  1  2     Finally, let’s look at the sentence rankings:\nsentences \u0026lt;- unique(job_des[, c(\u0026quot;sentence_id\u0026quot;, \u0026quot;sentence\u0026quot;)]) terminology \u0026lt;- job_des %\u0026gt;% filter(upos %in% c(\u0026quot;NOUN\u0026quot;, \u0026quot;VERB\u0026quot;, \u0026quot;ADJ\u0026quot;)) %\u0026gt;% select(sentence_id, lemma) tr \u0026lt;- textrank_sentences(data = sentences, terminology = terminology) kable(tr[[\u0026#39;sentences\u0026#39;]] %\u0026gt;% arrange(-textrank))    textrank_id  sentence  textrank      27  You are adept at data visualizations and have experience working with real-time data.4+ years of practical experience using data, models, and common sense to solve tough problems in a collaborative environment.  0.0703072    17  Your work will ensure that all our decisions are data-driven - this means that you will have a direct impact on the customer experience by influencing critical decisions on resource deployment and customer engagement.  0.0611901    11  Our work improves the customer experience, grows our market share and drives business outcomes.  0.0575264    16  This means hands-on work at all points in the data lifecycle, including data wrangling and mining; feature engineering; model-building and testing; and implementation and communication.  0.0567150    10  The data we have and the models we build are foundational to our platform and quite literally drive our business.  0.0509835    24  You are a hands-on learner and are excited by the thought of moving past theory and examples to real-time data science work.  0.0507275    30  Working experience with non-Excel BI tools such as Tableau, Looker, Superset, PowerBI, etc.You are comfortable building datasets using traditional relational databases and you’re familiar with alternative databases (noSQL, graph databases) or big data platforms such as Apache Spark.University degree in engineering, applied statistics, data mining, machine learning, mathematics or a related quantitative discipline.*  0.0445864    29  Working experience with Python, including Pandas, NumPy, scikit-learn, NLTK, and Keras/TensorFlow.An understanding of statistical and predictive modeling concepts, machine learning algorithms, clustering and classification techniques.Some exposure to one or more sub-fields of data science, especially GIS/spatial analysis; graph theory/network analysis; or natural language processing (NLP).  0.0434717    2  We’re building a team of innovators and disruptors to change the insurance experience to meet the demands of an on-demand world.  0.0430290    23  You are a collaborator and a communicator and are energized by working with multidisciplinary teams.  0.0420002    5  At Slice, we’re focused on ensuring our products provide a positive, individualized customer experience.  0.0394096    32  Data Science: 4 years (Required)Location:  0.0362884    28  Experience deploying models is a plus!  0.0350955    6  Our smaller teams invite broader thinking and problem solving, where nobody is pigeonholed into a predetermined role.  0.0342888    20  YOU are passionate about data.  0.0341904    14  Science work here at Slice is far from theoretical.  0.0332422    7  We work in an open, supportive, environment that values and promotes inclusiveness, innovation, and collaboration.  0.0307840    31  Job Types: Full-time, Permanent Experience:  0.0290179    12  This is an opportunity to play an active part in delivering our data vision and determining how we get there.  0.0280136    4  We believe it can and should be all digital and on-demand, so customers get only the coverage they need, right when they need it.  0.0270914    18  Our immediate applications include marketing analytics, fraud prevention, and risk/value modeling.  0.0221562    21  You enjoy being able to combine your analytical, technical and business skills in one role.  0.0211016    9  *The Data Science team is key to Slice’s ongoing success.  0.0184101    3  We are disrupting the idea that insurance needs to be a fixed product, with a fixed term, with fixed coverage.  0.0182936    15  This ROLE develops and executes Data Science projects across the company.  0.0152376    19  (Our future applications are boundless!)  0.0120358    22  You love solving problems and predicting behaviours.  0.0114608    26  An effective communication style with an ability to translate “the complex” to “the simple”.  0.0072589    1  WE are Slice Labs (Slice).  0.0052174    8  It’s fast-paced, dynamic and fulfilling.  0.0052174    13  All that to say: Data  0.0052174    25  You bring:  0.0052174    33  Ottawa, ON (Preferred)  0.0052174     library(magrittr) x \u0026lt;- tr[[\u0026#39;sentences\u0026#39;]] %\u0026gt;% arrange(-textrank) %\u0026gt;% head(10) Not bad, the top sentences are the big picture stuff that might give an overall impression of the job.\nAnd, just out of interest:\ntr[[\u0026quot;sentences\u0026quot;]] %\u0026gt;% ggplot(aes(textrank_id, textrank, fill = textrank_id)) + geom_col(color=\u0026quot;blue\u0026quot;, fill=rgb(0.1,0.4,0.5,0.7)) + theme_classic() + guides(fill = \u0026quot;none\u0026quot;) + labs(x = \u0026quot;Sentence\u0026quot;, y = \u0026quot;TextRank score\u0026quot;)  Conclusion textrank is a nifty little package for simple extractive text summary. However, it is important to think carefully about the type of summary you are aiming for, and whether this type of extractive summaries by the textrank algorithm can accomplish it.\nPros\n Easy to use\n  Summaries make sense at a glance\n \nCons\n Hard to know if these summaries really are of good quality, as there are no easy ways to validate the quality of the summaries\n  The measure of similarities between sentences, namely the number of words that two sentences have in common, is quite simplistic, and so can only produce “summaries” in a certain sense\n   pytextrank import spacy import pytextrank # example text text = r.job_des_raw # load a spaCy model, depending on language, scale, etc. nlp = spacy.load(\u0026quot;en_core_web_sm\u0026quot;) # add PyTextRank to the spaCy pipeline tr = pytextrank.TextRank() nlp.add_pipe(tr.PipelineComponent, name=\u0026quot;textrank\u0026quot;, last=True) doc = nlp(text) # examine the top-ranked phrases in the document pytextrank = [] for c in list(doc._.textrank.summary(limit_phrases=10, limit_sentences=10)): pytextrank.append(str(c)) print(\u0026#39;---\u0026#39;) ## --- ## --- ## --- ## --- ## --- ## --- ## --- ## --- ## --- ## --- for p in doc._.phrases[:10]: print(\u0026#39;{:.4f} {:5d} {}\u0026#39;.format(p.rank, p.count, p.text)) print(p.chunks) ## 0.0789 1 data mining ## [data mining] ## 0.0768 1 data science ## [data science] ## 0.0764 3 data ## [data, data, The data] ## 0.0762 1 experience deploying models ## [Experience deploying models] ## 0.0746 1 data visualizations ## [data visualizations] ## 0.0741 1 big data platforms ## [big data platforms] ## 0.0739 1 data wrangling ## [data wrangling] ## 0.0725 1 experience ## [experience] ## 0.0717 1 nlp).working experience ## [NLP).Working experience] ## 0.0713 1 practical experience ## [practical experience]  gensim from gensim.summarization.summarizer import summarize for sent in summarize(text, split=True, ratio=0.3): print(sent) print(\u0026#39;-\u0026#39;) ## We’re building a team of innovators and disruptors to change the insurance experience to meet the demands of an on-demand world. ## - ## The data we have and the models we build are foundational to our platform and quite literally drive our business. ## - ## Our work improves the customer experience, grows our market share and drives business outcomes. ## - ## All that to say: Data Science work here at Slice is far from theoretical. ## - ## This means hands-on work at all points in the data lifecycle, including data wrangling and mining; feature engineering; model-building and testing; and implementation and communication. ## - ## Your work will ensure that all our decisions are data-driven - this means that you will have a direct impact on the customer experience by influencing critical decisions on resource deployment and customer engagement. ## - ## You are a collaborator and a communicator and are energized by working with multidisciplinary teams. ## - ## You are a hands-on learner and are excited by the thought of moving past theory and examples to real-time data science work. ## - ## You are adept at data visualizations and have experience working with real-time data.4+ years of practical experience using data, models, and common sense to solve tough problems in a collaborative environment. ## -  summa from summa.summarizer import summarize for sent in summarize(text, split=True, ratio=0.3): print(sent) print(\u0026#39;-\u0026#39;) ## *The Data Science team is key to Slice’s ongoing success. ## - ## The data we have and the models we build are foundational to our platform and quite literally drive our business. ## - ## Our work improves the customer experience, grows our market share and drives business outcomes. ## - ## All that to say: Data Science work here at Slice is far from theoretical. ## - ## This means hands-on work at all points in the data lifecycle, including data wrangling and mining; feature engineering; model-building and testing; and implementation and communication. ## - ## Your work will ensure that all our decisions are data-driven - this means that you will have a direct impact on the customer experience by influencing critical decisions on resource deployment and customer engagement. ## - ## You are a hands-on learner and are excited by the thought of moving past theory and examples to real-time data science work. ## - ## You are adept at data visualizations and have experience working with real-time data.4+ years of practical experience using data, models, and common sense to solve tough problems in a collaborative environment. ## - ## Experience deploying models is a plus!Working experience with Python, including Pandas, NumPy, scikit-learn, NLTK, and Keras/TensorFlow.An understanding of statistical and predictive modeling concepts, machine learning algorithms, clustering and classification techniques.Some exposure to one or more sub-fields of data science, especially GIS/spatial analysis; graph theory/network analysis; or natural language processing (NLP).Working experience with non-Excel BI tools such as Tableau, Looker, Superset, PowerBI, etc.You are comfortable building datasets using traditional relational databases and you’re familiar with alternative databases (noSQL, graph databases) or big data platforms such as Apache Spark.University degree in engineering, applied statistics, data mining, machine learning, mathematics or a related quantitative discipline.* ## -  Summary res = r.x z = res[\u0026#39;sentence\u0026#39;].tolist() import pandas as pd import collections gensim = summarize(text, split=True, ratio=0.3) summa = summarize(text, split=True, ratio=0.3) a = pytextrank + gensim + summa + z counter=collections.Counter(a) pd.DataFrame(counter, index=[0]).T ## 0 ## You are adept at data visualizations and have e... 4 ## Some exposure to one or more sub-fields of data... 1 ## University degree in engineering, applied stati... 1 ## The Data Science team is key to Slice’s ongoing... 1 ## YOU are passionate about data. 1 ## You are comfortable building datasets using tra... 1 ## This means hands-on work at all points in the d... 4 ## WE are Slice Labs (Slice). 1 ## We’re building a team of innovators and disrupt... 2 ## We are disrupting the idea that insurance needs... 1 ## *The Data Science team is key to Slice’s ongoin... 2 ## The data we have and the models we build are fo... 3 ## Our work improves the customer experience, grow... 3 ## All that to say: Data Science work here at Slic... 2 ## Your work will ensure that all our decisions ar... 3 ## You are a hands-on learner and are excited by t... 3 ## Experience deploying models is a plus!Working e... 2 ## Working experience with non-Excel BI tools such... 1 ## Working experience with Python, including Panda... 1 ## You are a collaborator and a communicator and a... 1 summa_df = pd.DataFrame({\u0026#39;phrases\u0026#39;: summa, \u0026#39;summa\u0026#39;: [1]*len(summa)}) #summa_df.set_index(\u0026#39;phrases\u0026#39;) gensim_df = pd.DataFrame({\u0026#39;phrases\u0026#39;: gensim, \u0026#39;gensim\u0026#39;: [1]*len(gensim)}) #gensim_df.set_index(\u0026#39;phrases\u0026#39;) pytextrank_df = pd.DataFrame({\u0026#39;phrases\u0026#39;: pytextrank, \u0026#39;pytextrank\u0026#39;: [1]*len(pytextrank)}) #pytextrank_df.set_index(\u0026#39;phrases\u0026#39;) textrank_df = pd.DataFrame({\u0026#39;phrases\u0026#39;: z, \u0026#39;r_textrank\u0026#39;: [1]*len(z)}) from functools import reduce v = reduce(lambda left,right: pd.merge(left,right,on=\u0026#39;phrases\u0026#39;, how=\u0026#39;outer\u0026#39;), [summa_df, gensim_df, pytextrank_df, textrank_df]) v = v.fillna(\u0026#39;0\u0026#39;) v = v.set_index(\u0026#39;phrases\u0026#39;) for x in v: v[x] = pd.to_numeric(v[x]) import seaborn as sns import matplotlib.pyplot as plt g=sns.clustermap(v) g.savefig(\u0026quot;output.png\u0026quot;) plt.show() k \u0026lt;- py$v kable(k)     summa  gensim  pytextrank  r_textrank      *The Data Science team is key to Slice’s ongoing success.  1  1  0  0    The data we have and the models we build are foundational to our platform and quite literally drive our business.  1  1  0  1    Our work improves the customer experience, grows our market share and drives business outcomes.  1  1  0  1    All that to say: Data Science work here at Slice is far from theoretical.  1  1  0  0    This means hands-on work at all points in the data lifecycle, including data wrangling and mining; feature engineering; model-building and testing; and implementation and communication.  1  1  1  1    Your work will ensure that all our decisions are data-driven - this means that you will have a direct impact on the customer experience by influencing critical decisions on resource deployment and customer engagement.  1  1  0  1    You are a hands-on learner and are excited by the thought of moving past theory and examples to real-time data science work.  1  1  0  1    You are adept at data visualizations and have experience working with real-time data.4+ years of practical experience using data, models, and common sense to solve tough problems in a collaborative environment.  1  1  1  1    Experience deploying models is a plus!Working experience with Python, including Pandas, NumPy, scikit-learn, NLTK, and Keras/TensorFlow.An understanding of statistical and predictive modeling concepts, machine learning algorithms, clustering and classification techniques.Some exposure to one or more sub-fields of data science, especially GIS/spatial analysis; graph theory/network analysis; or natural language processing (NLP).Working experience with non-Excel BI tools such as Tableau, Looker, Superset, PowerBI, etc.You are comfortable building datasets using traditional relational databases and you’re familiar with alternative databases (noSQL, graph databases) or big data platforms such as Apache Spark.University degree in engineering, applied statistics, data mining, machine learning, mathematics or a related quantitative discipline.*  1  1  0  0    Some exposure to one or more sub-fields of data science, especially GIS/spatial analysis; graph theory/network analysis; or natural language processing (NLP).Working experience with non-Excel BI tools such as Tableau, Looker, Superset, PowerBI, etc.  0  0  1  0    University degree in engineering, applied statistics, data mining, machine learning, mathematics or a related quantitative discipline.  0  0  1  0    The Data Science team is key to Slice’s ongoing success.  0  0  1  0    YOU are passionate about data.  0  0  1  0    You are comfortable building datasets using traditional relational databases and you’re familiar with alternative databases (noSQL, graph databases) or big data platforms such as Apache Spark.  0  0  1  0    WE are Slice Labs (Slice).  0  0  1  0    We’re building a team of innovators and disruptors to change the insurance experience to meet the demands of an on-demand world.  0  0  1  1    We are disrupting the idea that insurance needs to be a fixed product, with a fixed term, with fixed coverage.  0  0  1  0    Working experience with non-Excel BI tools such as Tableau, Looker, Superset, PowerBI, etc.You are comfortable building datasets using traditional relational databases and you’re familiar with alternative databases (noSQL, graph databases) or big data platforms such as Apache Spark.University degree in engineering, applied statistics, data mining, machine learning, mathematics or a related quantitative discipline.*  0  0  0  1    Working experience with Python, including Pandas, NumPy, scikit-learn, NLTK, and Keras/TensorFlow.An understanding of statistical and predictive modeling concepts, machine learning algorithms, clustering and classification techniques.Some exposure to one or more sub-fields of data science, especially GIS/spatial analysis; graph theory/network analysis; or natural language processing (NLP).  0  0  0  1    You are a collaborator and a communicator and are energized by working with multidisciplinary teams.  0  0  0  1      sumy from __future__ import absolute_import from __future__ import division, print_function, unicode_literals from sumy.parsers.html import HtmlParser from sumy.parsers.plaintext import PlaintextParser from sumy.nlp.tokenizers import Tokenizer from sumy.summarizers.lsa import LsaSummarizer as Summarizer from sumy.nlp.stemmers import Stemmer from sumy.utils import get_stop_words def sumy_function(url, SENTENCES_COUNT = 9, LANGUAGE = \u0026quot;english\u0026quot;): parser = HtmlParser.from_url(url, Tokenizer(LANGUAGE)) # or for plain text files # parser = PlaintextParser.from_file(\u0026quot;document.txt\u0026quot;, Tokenizer(LANGUAGE)) # parser = PlaintextParser.from_string(\u0026quot;Check this out.\u0026quot;, Tokenizer(LANGUAGE)) stemmer = Stemmer(LANGUAGE) summarizer = Summarizer(stemmer) summarizer.stop_words = get_stop_words(LANGUAGE) for sentence in summarizer(parser.document, SENTENCES_COUNT): print(sentence) print(\u0026#39;\\n\u0026#39;) sumy_function(\u0026quot;https://ca.indeed.com/viewjob?cmp=Slice-Insurance-Technologies-Inc.\u0026amp;t=Data+Scientist\u0026amp;jk=35c9914ee9b0a052\u0026amp;sjdu=vQIlM60yK_PwYat7ToXhk3myaatk1OsAkhQIw2UEit1eiAfw8fAJl6BKuZ5V5P0TgQjo7h2qNGvAZ_80Lr3XxA\u0026amp;tk=1e1nuvuml0np3000\u0026amp;adid=335410488\u0026amp;pub=4a1b367933fd867b19b072952f68dceb\u0026amp;vjs=3\u0026quot;) ## We’re building a team of innovators and disruptors to change the insurance experience to meet the demands of an on-demand world. ## ## ## At Slice, we’re focused on ensuring our products provide a positive, individualized customer experience. ## ## ## Our smaller teams invite broader thinking and problem solving, where nobody is pigeonholed into a predetermined role. ## ## ## We work in an open, supportive, environment that values and promotes inclusiveness, innovation, and collaboration. ## ## ## Our work improves the customer experience, grows our market share and drives business outcomes. ## ## ## You are a hands-on learner and are excited by the thought of moving past theory and examples to real-time data science work. ## ## ## 4+ years of practical experience using data, models, and common sense to solve tough problems in a collaborative environment. ## ## ## An understanding of statistical and predictive modeling concepts, machine learning algorithms, clustering and classification techniques. ## ## ## University degree in engineering, applied statistics, data mining, machine learning, mathematics or a related quantitative discipline.       https://rdrr.io/cran/textrank/man/textrank_sentences.html↩\n   ","permalink":"/post/extractive-summarization/","tags":["R","Python","Package","textrank","Text summarization"],"title":"Extractive text summarization"},{"categories":["Data science toolbox"],"contents":"       \n --       \n One of the most exciting things about data science is when you get your hands on a new dataset. Oh, the sense of possibilities when opening up a new dataset!\nUnfortunately, before you can get to the fun stuff (though who said that EDA can’t be fun), it’s important to get an idea of its overall structure and potential problems. Here is a round up of our favourite packages for getting acquainted with a dataset while writing a minimum amount of code.\nesquisse: interactive data exploration with ggplot2 If you are really impatient, esquisse is an RStudio addin that launches a point-and-click GUI for absolutely no-code interactive EDA. After drag-and-drop selection of the features that you want to visualize, it not only generates customized beautiful ggplot2 figures but also exports the code so that you can easily replicate them elsewhere.\nFrom its official documentation:\nesquisse can also be used as a module inside your Shiny application.\n dataMaid: quality check of raw data To quickly spot things like missing values, misclassified variables, and erroneous values, I prefer dataMaid for its straight forward combination of metrics and visualizations.\n## Import library library(\u0026#39;dataMaid\u0026#39;) ## Import data raw Telco customer churn dataset raw_df \u0026lt;- read.csv(\u0026quot;https://github.com/treselle-systems/customer_churn_analysis/raw/master/WA_Fn-UseC_-Telco-Customer-Churn.csv\u0026quot;) dataMaid generates a summary report of your dataset in R markdown format, which you can knit together into an PDF or HTML report. For demonstration purposes, I will just show snippets of the interesting parts:\n## Generate report makeDataReport(raw_df, openResult = TRUE, output=\u0026#39;html\u0026#39;, render = TRUE, file = \u0026quot;./auto_eda_report.Rmd\u0026quot;, replace = TRUE, codebook=TRUE) First part of the generated report shows the types of checks performed:\n   Then, we see a summary table of all variables, which provides a helpful quick overview of the data and any potential issues, like the 0.16% missing data in the TotalCharges column.\n   Scrolling down, there are more detailed information on each variable. We see problematic areas such as the customerID column being a key and that the SeniorCitizen column is encoded in 0s and 1s.\n   Also we see that the minimum value of Tenure column is 0, which is problematic and should be removed.\n   Of all the automated EDA packages in R and Python that I have tried so far, dataMaid provides the best once-over, quick-glance view of the whole dataset with a single function. These results are great for focusing the initial data cleaning process.\n autoEDA: quick overview of cleaned data Once I get a (reasonably) clean data set, I want to be able to explore the variables and their relationships with minimal coding (at first). This is where the next two packages come in, which provide varying degrees of flexibility and depth of insights.\nFor the first quick overview, I use the autoEDA package to explore the relationship between all input variables and my target variable of interest, which is Churn in this case. For maximum convenience, this is can be done in a single line of code:\n## Import libraries library(autoEDA) ## Import the same dataset, but with basic cleaning cleaned_df \u0026lt;- read.csv(\u0026quot;https://github.com/nchelaru/data-prep/raw/master/telco_cleaned_yes_no.csv\u0026quot;) ## Correctly format the target variable cleaned_df$Churn \u0026lt;- as.character(cleaned_df$Churn) ## autoEDA autoEDA_results \u0026lt;- autoEDA(cleaned_df, y = \u0026quot;Churn\u0026quot;, returnPlotList = TRUE, verbose = FALSE)  The graphical outputs provided by autoEDA give very quick at-a-glance insights into how various aspects of customer demographics and behaviour relate to whether they churn or not. As there are many plots, one for each variable plus some more, I will show them in a nifty carousel made possible by the slickR package:\n## Import libraries library(svglite) library(lattice) library(ggplot2) library(rvest) library(reshape2) library(dplyr) library(htmlwidgets) library(slickR) ## Create list of autoEDA figures converted to SVG plotsToSVG \u0026lt;- list() i \u0026lt;- 1 for (v in autoEDA_results$plots) { x \u0026lt;- xmlSVG({show(v)}, standalone=TRUE) plotsToSVG[[i]] \u0026lt;- x i \u0026lt;- i +1 } ## Custom function needed to render SVGs in Chrome/Firefox hash_encode_url \u0026lt;- function(url){ gsub(\u0026quot;#\u0026quot;, \u0026quot;%23\u0026quot;, url) } ## Pass list of figures to SlickR s.in \u0026lt;- sapply(plotsToSVG, function(sv){hash_encode_url(paste0(\u0026quot;data:image/svg+xml;utf8,\u0026quot;,as.character(sv)))}) slickR(s.in, slideId = \u0026#39;ex4\u0026#39;,slickOpts = list(dots=T), width = \u0026#39;100%\u0026#39;) #htmlwidget-8f1773b6fe181ad14286 {margin-left:auto;margin-right:auto}  {\"x\":[{\"divName\":\"ex4\",\"divType\":\"img\",\"padding\":\"99%\",\"obj\":[\"data:image/svg+xml;utf8,\\n\\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n 0%\\n \\n \\n 20%\\n \\n \\n 40%\\n \\n \\n 60%\\n \\n \\n \\n \\n \\n \\n \\n \\n \\n No\\n \\n \\n Yes\\n \\n \\n Churn\\n \\n \\n Relative Frequency\\n \\n \\n \\n Churn\\n \\n \\n \\n \\n No\\n \\n \\n Yes\\n \\n \\n Outcome distribution\\n \\n\\n\",\"data:image/svg+xml;utf8,\\n\\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n 0%\\n \\n \\n 25%\\n \\n \\n 50%\\n \\n \\n 75%\\n \\n \\n 100%\\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n No\\n \\n \\n Yes\\n \\n \\n Churn\\n \\n \\n Relative Frequency\\n \\n \\n \\n Gender\\n \\n \\n \\n \\n FEMALE\\n \\n \\n MALE\\n \\n \\n Distribution: Churn By Gender\\n \\n\\n\",\"data:image/svg+xml;utf8,\\n\\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n 0%\\n \\n \\n 25%\\n \\n \\n 50%\\n \\n \\n 75%\\n \\n \\n 100%\\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n No\\n \\n \\n Yes\\n \\n \\n Churn\\n \\n \\n Relative Frequency\\n \\n \\n \\n SeniorCitizen\\n \\n \\n \\n \\n NO\\n \\n \\n YES\\n \\n \\n Distribution: Churn By SeniorCitizen\\n \\n\\n\",\"data:image/svg+xml;utf8,\\n\\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n 0%\\n \\n \\n 25%\\n \\n \\n 50%\\n \\n \\n 75%\\n \\n \\n 100%\\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n No\\n \\n \\n Yes\\n \\n \\n Churn\\n \\n \\n Relative Frequency\\n \\n \\n \\n Partner\\n \\n \\n \\n \\n NO\\n \\n \\n YES\\n \\n \\n Distribution: Churn By Partner\\n \\n\\n\",\"data:image/svg+xml;utf8,\\n\\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n 0%\\n \\n \\n 25%\\n \\n \\n 50%\\n \\n \\n 75%\\n \\n \\n 100%\\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n No\\n \\n \\n Yes\\n \\n \\n Churn\\n \\n \\n Relative Frequency\\n \\n \\n \\n Dependents\\n \\n \\n \\n \\n NO\\n \\n \\n YES\\n \\n \\n Distribution: Churn By Dependents\\n \\n\\n\",\"data:image/svg+xml;utf8,\\n\\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n 0\\n \\n \\n 200\\n \\n \\n 400\\n \\n \\n 600\\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n 0\\n \\n \\n 20\\n \\n \\n 40\\n \\n \\n 60\\n \\n \\n Tenure\\n \\n \\n Frequency\\n \\n \\n \\n Churn\\n \\n \\n \\n \\n No\\n \\n \\n Yes\\n \\n \\n Distribution: Tenure By Churn\\n \\n\\n\",\"data:image/svg+xml;utf8,\\n\\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n 0%\\n \\n \\n 25%\\n \\n \\n 50%\\n \\n \\n 75%\\n \\n \\n 100%\\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n No\\n \\n \\n Yes\\n \\n \\n Churn\\n \\n \\n Relative Frequency\\n \\n \\n \\n PhoneService\\n \\n \\n \\n \\n NO\\n \\n \\n YES\\n \\n \\n Distribution: Churn By PhoneService\\n \\n\\n\",\"data:image/svg+xml;utf8,\\n\\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n 0%\\n \\n \\n 25%\\n \\n \\n 50%\\n \\n \\n 75%\\n \\n \\n 100%\\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n No\\n \\n \\n Yes\\n \\n \\n Churn\\n \\n \\n Relative Frequency\\n \\n \\n \\n MultipleLines\\n \\n \\n \\n \\n NO\\n \\n \\n YES\\n \\n \\n Distribution: Churn By MultipleLines\\n \\n\\n\",\"data:image/svg+xml;utf8,\\n\\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n 0%\\n \\n \\n 25%\\n \\n \\n 50%\\n \\n \\n 75%\\n \\n \\n 100%\\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n No\\n \\n \\n Yes\\n \\n \\n Churn\\n \\n \\n Relative Frequency\\n \\n \\n \\n InternetService\\n \\n \\n \\n \\n \\n DSL\\n \\n \\n FIBER OPTIC\\n \\n \\n NO\\n \\n \\n Distribution: Churn By InternetService\\n \\n\\n\",\"data:image/svg+xml;utf8,\\n\\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n 0%\\n \\n \\n 25%\\n \\n \\n 50%\\n \\n \\n 75%\\n \\n \\n 100%\\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n No\\n \\n \\n Yes\\n \\n \\n Churn\\n \\n \\n Relative Frequency\\n \\n \\n \\n OnlineSecurity\\n \\n \\n \\n \\n NO\\n \\n \\n YES\\n \\n \\n Distribution: Churn By OnlineSecurity\\n \\n\\n\",\"data:image/svg+xml;utf8,\\n\\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n 0%\\n \\n \\n 25%\\n \\n \\n 50%\\n \\n \\n 75%\\n \\n \\n 100%\\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n No\\n \\n \\n Yes\\n \\n \\n Churn\\n \\n \\n Relative Frequency\\n \\n \\n \\n OnlineBackup\\n \\n \\n \\n \\n NO\\n \\n \\n YES\\n \\n \\n Distribution: Churn By OnlineBackup\\n \\n\\n\",\"data:image/svg+xml;utf8,\\n\\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n 0%\\n \\n \\n 25%\\n \\n \\n 50%\\n \\n \\n 75%\\n \\n \\n 100%\\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n No\\n \\n \\n Yes\\n \\n \\n Churn\\n \\n \\n Relative Frequency\\n \\n \\n \\n DeviceProtection\\n \\n \\n \\n \\n NO\\n \\n \\n YES\\n \\n \\n Distribution: Churn By DeviceProtection\\n \\n\\n\",\"data:image/svg+xml;utf8,\\n\\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n 0%\\n \\n \\n 25%\\n \\n \\n 50%\\n \\n \\n 75%\\n \\n \\n 100%\\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n No\\n \\n \\n Yes\\n \\n \\n Churn\\n \\n \\n Relative Frequency\\n \\n \\n \\n TechSupport\\n \\n \\n \\n \\n NO\\n \\n \\n YES\\n \\n \\n Distribution: Churn By TechSupport\\n \\n\\n\",\"data:image/svg+xml;utf8,\\n\\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n 0%\\n \\n \\n 25%\\n \\n \\n 50%\\n \\n \\n 75%\\n \\n \\n 100%\\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n No\\n \\n \\n Yes\\n \\n \\n Churn\\n \\n \\n Relative Frequency\\n \\n \\n \\n StreamingTV\\n \\n \\n \\n \\n NO\\n \\n \\n YES\\n \\n \\n Distribution: Churn By StreamingTV\\n \\n\\n\",\"data:image/svg+xml;utf8,\\n\\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n 0%\\n \\n \\n 25%\\n \\n \\n 50%\\n \\n \\n 75%\\n \\n \\n 100%\\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n No\\n \\n \\n Yes\\n \\n \\n Churn\\n \\n \\n Relative Frequency\\n \\n \\n \\n StreamingMovies\\n \\n \\n \\n \\n NO\\n \\n \\n YES\\n \\n \\n Distribution: Churn By StreamingMovies\\n \\n\\n\",\"data:image/svg+xml;utf8,\\n\\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n 0%\\n \\n \\n 25%\\n \\n \\n 50%\\n \\n \\n 75%\\n \\n \\n 100%\\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n No\\n \\n \\n Yes\\n \\n \\n Churn\\n \\n \\n Relative Frequency\\n \\n \\n \\n Contract\\n \\n \\n \\n \\n \\n MONTH-TO-MONTH\\n \\n \\n ONE YEAR\\n \\n \\n TWO YEAR\\n \\n \\n Distribution: Churn By Contract\\n \\n\\n\",\"data:image/svg+xml;utf8,\\n\\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n 0%\\n \\n \\n 25%\\n \\n \\n 50%\\n \\n \\n 75%\\n \\n \\n 100%\\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n No\\n \\n \\n Yes\\n \\n \\n Churn\\n \\n \\n Relative Frequency\\n \\n \\n \\n PaperlessBilling\\n \\n \\n \\n \\n NO\\n \\n \\n YES\\n \\n \\n Distribution: Churn By PaperlessBilling\\n \\n\\n\",\"data:image/svg+xml;utf8,\\n\\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n 0%\\n \\n \\n 25%\\n \\n \\n 50%\\n \\n \\n 75%\\n \\n \\n 100%\\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n No\\n \\n \\n Yes\\n \\n \\n Churn\\n \\n \\n Relative Frequency\\n \\n \\n \\n PaymentMethod\\n \\n \\n \\n \\n \\n \\n BANK TRANSFER (AUTOMATIC)\\n \\n \\n CREDIT CARD (AUTOMATIC)\\n \\n \\n ELECTRONIC CHECK\\n \\n \\n MAILED CHECK\\n \\n \\n Distribution: Churn By PaymentMethod\\n \\n\\n\",\"data:image/svg+xml;utf8,\\n\\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n 0\\n \\n \\n 250\\n \\n \\n 500\\n \\n \\n 750\\n \\n \\n 1000\\n \\n \\n 1250\\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n 25\\n \\n \\n 50\\n \\n \\n 75\\n \\n \\n 100\\n \\n \\n MonthlyCharges\\n \\n \\n Frequency\\n \\n \\n \\n Churn\\n \\n \\n \\n \\n No\\n \\n \\n Yes\\n \\n \\n Distribution: MonthlyCharges By Churn\\n \\n\\n\",\"data:image/svg+xml;utf8,\\n\\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n 0\\n \\n \\n 500\\n \\n \\n 1000\\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n 0\\n \\n \\n 2500\\n \\n \\n 5000\\n \\n \\n 7500\\n \\n \\n TotalCharges\\n \\n \\n Frequency\\n \\n \\n \\n Churn\\n \\n \\n \\n \\n No\\n \\n \\n Yes\\n \\n \\n Distribution: TotalCharges By Churn\\n \\n\\n\",\"data:image/svg+xml;utf8,\\n\\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n 0%\\n \\n \\n 25%\\n \\n \\n 50%\\n \\n \\n 75%\\n \\n \\n \\n \\n \\n \\n \\n \\n \\n Low\\n \\n \\n Medium\\n \\n \\n PredictivePower\\n \\n \\n Relative Frequency\\n \\n \\n \\n PredictivePower\\n \\n \\n \\n \\n Low\\n \\n \\n Medium\\n \\n \\n Predictive power of features\\n \\n\\n\"],\"slickOpts\":{\"dots\":true}}],\"evals\":[],\"jsHooks\":[]} \nIt is important to keep in mind that this type of bivariate analysis cannot detect combinatorial effects that exist among multiple variables to affect churn. Therefore, just because a variable do not appear to be differently distributed in terms of churn behaviour, such as Gender, it should not be excluded from analysis as it may be significant when considered in combination with other variables. Nevertheless, this is a good start for seeing if there are “learnable” signals in the dataset.\nThe output also includes a dataframe with summary statistics pertaining to variable type, presence of outliers, and descriptive statistics.\n## Import libraries library(knitr) library(kableExtra) ## Preview data kable(t(head(autoEDA_results$overview, 4)), colnames=NULL) %\u0026gt;% kable_styling(bootstrap_options = c(\u0026quot;striped\u0026quot;, \u0026quot;hover\u0026quot;))    1  2  3  4      Feature  Churn  Contract  Dependents  DeviceProtection    Observations  7032  7032  7032  7032    FeatureClass  character  character  character  character    FeatureType  Categorical  Categorical  Categorical  Categorical    PercentageMissing  0  0  0  0    PercentageUnique  0.03  0.04  0.03  0.03    ConstantFeature  No  No  No  No    ZeroSpreadFeature  No  No  No  No    LowerOutliers  0  0  0  0    UpperOutliers  0  0  0  0    ImputationValue  NO  MONTH-TO-MONTH  NO  NO    MinValue  0  0  0  0    FirstQuartile  0  0  0  0    Median  0  0  0  0    Mean  0  0  0  0    Mode  NO  MONTH-TO-MONTH  NO  NO    ThirdQuartile  0  0  0  0    MaxValue  0  0  0  0    LowerOutlierValue  0  0  0  0    UpperOutlierValue  0  0  0  0    PredictivePowerPercentage  0  46  17  7    PredictivePower  Low  Medium  Low  Low     In the last row, there is a handy PredictivePower metric for each input variable with respect to a specified target variable. For now, we can ignore this as I will cover it in more details in a later post examining variable importance.\n ExPanDaR: your own Shiny app for data exploration ExPanDaR provides a really nifty Shiny app for interactive explorations of your data set. Originally designed for examining time-series data, the package requires the input dataframe to have a 1) time/date column and 2) a column that uniquely identifies each row. As the time/date column is only needed if you want to visualize time-dependent trends, to use a dataset without a time dimension you can just add a new numeric column (ts) with a constant and set that as the time dimension. An index column would suffice for the second requirement. In the original Telco dataset, the customerID column would have worked fine. As I had dropped it in the process of data cleaning, I will just add a new index column (ID).\n## Import library library(ExPanDaR) ## Add mock time column and new index to dataframe cleaned_df$ts \u0026lt;- rep(1, nrow(cleaned_df)) cleaned_df$ID \u0026lt;- seq.int(nrow(cleaned_df)) To start up the Shiny app for interactive exploration of the results:\nExPanD(df = cleaned_df, cs_id = \u0026quot;ID\u0026quot;, ts_id = \u0026quot;ts\u0026quot;) Here are some snapshots of the features that I find most useful. The dropdown menus and sliders make it really easy and flexible to examine any combinations of variables.\n         To go beyond bivariate relationships, the scatter plot can aggregate information from up to four variables and really give some interesting insights.\n   There are some other very cool features like allowing the user to generate and explore new variables (from some arithemtic combinations of existing variables) on the fly and performing regression analysis. Definitely give this package a try!\n     ","permalink":"/post/fast-exploratory-data-analysis-for-the-impatient/","tags":["EDA","R","Visualizations"],"title":"Fast exploratory data analysis for when you just can't wait"},{"categories":["Data science toolbox"],"contents":" \n --       \n As useful as they are, and they really are, Jupyter notebooks can feel rather stale after a few years. While they are great for quickly testing out code and exploring datasets, I can’t help but want something more fun and polished for presenting a completed project.\nFor this reason I had been working largely in R for the past while, despite my preference for the simplicity of the Python syntax, in large part due to the vibrant Shiny ecosystem that makes creating dashboards and interactive web apps easy and fun. However, I am happy to report that in really just the past year or so, the interactive app/dashboard scene in Python has really flourished, first with the appearance of the Plotly Dash platform and then most recently with Streamlit. Here is a (ever updating) round-up of my experiences so far with Python packages that allow us to bring our data science projects to life.\nStreamlit What really drew me back to Python is the appearance of Streamlit, an open-source library that really truly makes converting a data analysis workflow to an app a breeze. By adding a few magic commands, a Python script is spun to an interactive app that can be deployed on Heroku like any other web app.\n\n \nInitially, Streamlit seemed to me neither here nor there, sitting somewhere between Plotly Dash and Jupyter notebooks. While it seemed very easy to worked with, I thought that it was missing the “look” of Dash and also the versatility of cell-based operations of Jupyter. However, as soon as I gave it a try, I totally understood the allure.\nThe absolute best feature of Streamlit, in my opinion, is how easy it is to create interactive widgets like dropdown menus, radio boxes, sliders and even text/number inputs, without needing to write any callbacks. Using an example from the official documentation, this is how to create and get input from a slider:\nimport streamlit as st age = st.slider(\u0026#39;How old are you?\u0026#39;, 0, 130, 25) st.write(\u0026quot;I\u0026#39;m \u0026quot;, age, \u0026#39;years old\u0026#39;) Creating other types of interactive widgets in Streamlit is just as easy. You can find a list of functionalities currently supported here.\nIn comparison, this is how to create the same thing in Plotly Dash:\nimport dash import dash_html_components as html import dash_core_components as dcc app = dash.Dash(__name__, external_stylesheets=external_stylesheets) app.layout = html.Div([ dcc.Slider( id=\u0026#39;my-slider\u0026#39;, min=0, max=20, step=0.5, value=10, ), html.Div(id=\u0026#39;slider-output-container\u0026#39;) ]) @app.callback( dash.dependencies.Output(\u0026#39;slider-output-container\u0026#39;, \u0026#39;children\u0026#39;), [dash.dependencies.Input(\u0026#39;my-slider\u0026#39;, \u0026#39;value\u0026#39;)]) def update_output(value): return \u0026#39;You have selected \u0026quot;{}\u0026quot;\u0026#39;.format(value) if __name__ == \u0026#39;__main__\u0026#39;: app.run_server(debug=True)  Of course, Plotly Dash provides many other functionalities that Streamlit is not capable of, at least for now. However, the simplicity of working with Streamlit makes it so satistfying to quickly whip up an interactive app to showcase your work.\nFor example, I have made two Streamlit apps to host my microlearning series on survival analysis and building a random forest classifier to predict customer churn. I made them both multipage apps that allow progressive reveal of the content at the learner’s pace, in order to take advantage of the easy interactive widgets to the fullest extent. Granted that I had the workflow written out before hand, but making either one of these apps took only 2-3 days. Check out this one about integrating unsupervised and supervised machine learning!\n\n If you are interested in trying Streamlit out, there are several demo apps listed in the documentation linked above. In addition, many enthusiastic adopters of Streamlit have tweeted about their own creations.\n Plotly Dash Plotly Dash has been around for quite a while now, so I will not go as much in depth here, trusting that everyone is already pretty familiar with it. Unlike the other two packages introduced here, Dash has the benefit of the very large and active Plotly community to serve as a solid knowledge base to support users of all levels.\nAs of now, Plotly Dash just cannot be beat in terms of how polished its end products look. It is my package of choice if I need to create a dashboard/app that will be used by non-technical end users, such as business professionals, with clear interactive features and sophisticated crosstalk between elements (i.e. data tables, plots, maps, etc.). For example, here is a sales dashboard that I had made while learning the Dash platform.\n  However, as mentioned in comparison with Streamlit, the Dash code base can get quite large and complex very quickly, particularly when used with the built-in or Bootstrap grid system for layout. Consequently, it has a fairly steep learning curve, with very rewarding results. On a related note, I cannot recommend enough the Dash Bootstrap Components package, which greatly simplifies the implementations of a lot of layout and interactive features with the added benefit of the clean Bootstrap look.\nWant to get started on your own? For an step-by-step guide to building a professional dashboard, take a look at the video below made by a Plotly developer:\n \n Voilà Finally, just because we want to upgrade from Jupyter notebooks does not mean we are going to do away with it completely, as it is still a fantastic platform for exploring data and prototyping analysis workflows. In addition, the ability to use Python and R together in the same notebook makes it indispensible for data scientists who want the best of both worlds: ease of data wrangling in Python but mature analysis packages in R. Since so many of us begin a project in Jupyter notebooks, it would be a dream come true to be able to make dashboards/interactive apps from the analysis results right there. The recently released package Voilà grants that wish, somewhat.\nHere is an introduction to the package at SciPy 2019:\n\n \nWhile interactive widgets like dropdown menus and sliders can be added to the dashboard, as you can do in Streamlit and Plotly Dash, using the ipywidgets library, I have personally found the syntax much less clear and not as many tutorials/help pages available to get a newcomer started. In addition, as Voilà is still in very early stages of development, the resulting dashboard/app looks rather barebones as compared to Dash.\nHere is an “learning dashboard” that I had made using Voilà to introduce various model-agnostic approaches to calculate feature importance, for comparison with the sales dashboard made with Dash.\n  Nevertheless, as mentioned above, one strength of Jupyter notebooks is the ability to use other language kernels. Any language that is supported by a Jupyter kernel can be used to create a Voilà app, so for projects where that is needed, this would be the package for you.\n \nTaken together, it is really an exciting time for finally being able to communicate/present your Python data science projects in style. This post will be updated as new features and packages become available, so please check back once in a while! :)\n     ","permalink":"/post/making-python-apps/","tags":["Dash","Jupyter","Python","Streamlit","Voila"],"title":"Going beyond Jupyter notebooks"},{"categories":["Data science toolbox"],"contents":" \n --       \n We are always going to need Matplotlib and Seaborn, but it is so great to see new Python plotting packages popping up in the past few years. In addition to showcasing them here, we will document the little tricks and gotchas that we come across along the way.\nPlotly Express Plotly does not need much introduction, as it is now very widely used to create interactive plots in both Python and R. For a while, one major drawback of Plotly is its rather inconvenient syntax, where the source data needs to be passed in as arrays even though most of us want to be able to plot data directly from dataframes.\nWith the release of Plotly Express with its succinct Seaborn-like one-liner syntax, however, the situation is now much improved. While not all Plotly graphs can be made this way, there is an impressive variety of plots available in the Plotly Express library, including interactive maps.\nOne small annoyance is that the legends in graphs made using Plotly Express are cluttered by the column names. Following the useful tip here, you can use the .for_each_trace() function call to remove them. For example, here we will replace species= with an empty space:\nimport plotly.express as px iris = px.data.iris() fig = px.scatter(iris, x=\u0026quot;sepal_width\u0026quot;, y=\u0026quot;sepal_length\u0026quot;).for_each_trace(lambda t: t.update(name=t.name.replace(\u0026quot;species=\u0026quot;,\u0026quot;\u0026quot;)))    Here is a collection of useful links curated by us for all things Plotly:\n \nYellowbrick This package, built on top of Matplotlib, facilitates visualizing a variety of information from various stages of machine learning workflows. It is a sizable library, and looking through its API is an education in itself.\nWe have not had many chances to use Yellowbrick yet, but are looking forward to being able to delve deeper into this package in the future!\n\nPlotnine One of the strengths of the R language is its very powerful graphing package ggplot2, with its distinctive implementation of Leland Wilkinson’s Grammar of Graphics. The Plotnine package brings this (at least part of it, it seems for now) capability to Python, with very little change to the R syntax.\nThe brackets around the ggplot() function call looks strange at first, but it is needed for the signature multi-line ggplot2 grammer to work:\n## Import libraries import pandas as pd from plotnine import * from random import randint # Generate dataset random_numbers = [randint(1, 100) for p in range(0, 100)] df = pd.DataFrame({\u0026#39;number\u0026#39;: random_numbers}) # Draw plot p = ( ggplot(df, aes(x=\u0026#39;number\u0026#39;)) + geom_histogram(bins=20, na_rm=True) + ggtitle(\u0026#39;Histogram of random numbers\u0026#39;) + theme_light() ) ## Display plot p.draw(); If you want to save the plot to file:\np.save(\u0026quot;output.png\u0026quot;) And just because I can see myself wanting to use these plots in a Streamlit app, here is a working template:\nimport pandas as pd import numpy as np from plotnine import * import streamlit as st n = 10 df = pd.DataFrame({\u0026#39;x\u0026#39;: np.arange(n), \u0026#39;y\u0026#39;: np.arange(n), \u0026#39;yfit\u0026#39;: np.arange(n) + np.tile([-.2, .2], n // 2), \u0026#39;cat\u0026#39;: [\u0026#39;a\u0026#39;, \u0026#39;b\u0026#39;] * (n // 2)}) a = ( ggplot(df) + geom_col(aes(\u0026#39;x\u0026#39;, \u0026#39;y\u0026#39;)) ) fig = a.draw(); ## Needed to remove the \u0026quot;ggplot\u0026lt;#\u0026gt;\u0026quot; message st.pyplot() Finally, as always, here is a (ever growing) collection of links curated by us to get your started with using Plotnine:\n     ","permalink":"/post/graphing-in-python-a-walkthrough/","tags":["Python","Visualizations"],"title":"Graphing in Python - New(er) kids on the block"},{"categories":["Data science toolbox"],"contents":"       \n --       \n Business applications of data science is obviously a very broad topic, as data-driven approaches are becoming increasingly integrated into corporate practices. For this reason, this collection will begin as a scaffold of the topics that we are currently using or want to become familiar with in the near future. Be sure to check back regularly as we add add more content!\nGetting data    .accordion { background-color: #FCB97D; color: #F2CC8F; cursor: pointer; padding: 14px; width: 100%; border: none; text-align: left; outline: none; font-size: 13px; transition: 0.4s; } .active, .accordion:hover { background-color: #F2CC8F; } .panel { padding: 0 18px; background-color: white; max-height: 0; overflow: hidden; transition: max-height 0.2s ease-out; } h3 { padding-top: 0; font-size: 15px; } .content h3 { margin-bottom: 0; }     Working with Excel files    Read data from Excel file:\nimport pandas as pd df = pd.read_excel(\u0026#39;https://archive.ics.uci.edu/ml/machine-learning-databases/00352/Online%20Retail.xlsx\u0026#39;)  {\"x\":{\"tag\":{\"name\":\"Reactable\",\"attribs\":{\"data\":{\"InvoiceNo\":[[536365],[536365],[536365]],\"StockCode\":[[\"85123A\"],[71053],[\"84406B\"]],\"Description\":[[\"WHITE HANGING HEART T-LIGHT HOLDER\"],[\"WHITE METAL LANTERN\"],[\"CREAM CUPID HEARTS COAT HANGER\"]],\"Quantity\":[6,6,8],\"InvoiceDate\":[\"2010-12-01T08:26:00\",\"2010-12-01T08:26:00\",\"2010-12-01T08:26:00\"],\"UnitPrice\":[2.55,3.39,2.75],\"CustomerID\":[17850,17850,17850],\"Country\":[\"United Kingdom\",\"United Kingdom\",\"United Kingdom\"]},\"columns\":[{\"accessor\":\"InvoiceNo\",\"name\":\"InvoiceNo\",\"type\":\"list\"},{\"accessor\":\"StockCode\",\"name\":\"StockCode\",\"type\":\"list\"},{\"accessor\":\"Description\",\"name\":\"Description\",\"type\":\"list\"},{\"accessor\":\"Quantity\",\"name\":\"Quantity\",\"type\":\"numeric\"},{\"accessor\":\"InvoiceDate\",\"name\":\"InvoiceDate\",\"type\":\"Date\"},{\"accessor\":\"UnitPrice\",\"name\":\"UnitPrice\",\"type\":\"numeric\"},{\"accessor\":\"CustomerID\",\"name\":\"CustomerID\",\"type\":\"numeric\"},{\"accessor\":\"Country\",\"name\":\"Country\",\"type\":\"character\"}],\"defaultPageSize\":10,\"paginationType\":\"numbers\",\"showPageInfo\":true,\"minRows\":1,\"dataKey\":\"e3d7eb06698ef9fb96ced2163802ea0a\"},\"children\":[]},\"class\":\"reactR_markup\"},\"evals\":[],\"jsHooks\":[]} \n  Optical Character Recognition (OCR) of documents    Often, we need to extract data from print or scanned business documents, which is where these packages can come in handy:\n \n  Text parsing using regular expressions    To process OCR results into clean tabular data:\n   Data wrangling Given the topic, here we will focus on replicating common Excel functionalities/tasks in Python and R:\n Pivot tables    More coming soon! \n  Crosstab    More coming soon! \n  Sets    More coming soon! \n  Vlookup    More coming soon! \n  Analysis techniques This will be a collection of more “traditional” business analytics approaches. For machine learning methods, please see a future post devoted to the topic.\n Association rule mining    More coming soon! \n  RFM analysis    More coming soon! \n  Survival analysis    More coming soon! \n  Time-series analysis    More coming soon! \n  Reporting Good results are only useful when they are effectively communicated:\n Dashboards    More coming soon! \n  Presentation slides    More coming soon! \n  Automated reports    More coming soon! \n  var acc = document.getElementsByClassName(\"accordion\"); var i; for (i = 0; i        ","permalink":"/post/data-science-for-business/","tags":["Association rule mining","Python","R","Survival analysis"],"title":"Data science for business"},{"categories":["Dev handbook"],"contents":" \n --       \n  Where do we start with version control? It’s a bit finicky, often neglected, but oh-so important.\nOne of the key components of building good habits is to make the action easy to do. So here we round up the (usual) cheat sheets and some fun tools that make learning/remembering those git commands a little easier.\nCheat sheet Of course, to start us off, the official cheat sheet from Github:\n \nGit Explorer As promised, here is a fun one:\n     ","permalink":"/post/version-control-git-handbook/","tags":["Git"],"title":"Version control with Git"},{"categories":["Web development"],"contents":" \n       \n Making your own website or app is a great creative outlet. This is a continuously updated repository of free (or reasonably priced) resources for spicing up the UI/UX:\nIllustrations For those of us who are utterly not artistically inclined, jazz up landing pages and blog posts with these fantastic free illustration. Many of the images you see on this site are taken from these sites:\n \nIcons \u0026amp; Glyphs Pepper these through out to keep things interesting:\n \nColour schemes For those of us who know what looks good but can’t come up with it on our own, colour palette generators and inspirations are essential:\n     ","permalink":"/post/ui-and-ux-resources/","tags":["UI","UX"],"title":"Handy UI and UX resources "},{"categories":["Careers"],"contents":" \n --       \n There may exist some people in this world who enjoy job hunting, but we have not met one of them yet. As data scientist/developer jobs are becoming more and more popular, it is all the more important to know how to keep track of the most interesting opportunity and make your application to stand out. Here are a continuously updating suite of tools and services that we have tried or want to try for getting that next job.\nResumes One way to get yourself to stop putting off writing or updating your resume is make doing it fun and attractive. Remember the good old days of fiddling with Microsoft Word margins to get that two-column look just right? In recent years, almost too many online resume template/building platforms have popped up to keep track of. Here we have a collection of such platforms that offer at least some level of free services.\nFor creating the “official” resume to be send out, our favourites are CakeResume and Flow CV, as they offer the most innovative interfaces, generous free tiers and just the right amount of flexibility. For something a bit different, Qwilir offers templates to create a single-page web document to display the most salient points of your resume, which can be linked as part of a professional portfolio. Nevertheless, as everyone’s resume needs and aesthetic sensibilities differ, it might be worthwhile to look through each of the links below to find one that you particularly like.\n\n \nFinally, we have not tried it yet, but Resume Worded offers a free AI-powered review of your data science resume to check for important key words.\n    ","permalink":"/post/job-hunting-tools/","tags":["Job search","Resume","Useful"],"title":"Make job hunting fun again (kind of)"},{"categories":["Web development"],"contents":" \n --      h3 {font-size: 24px;}    \n Having made a few websites/web apps using Django, Blogdown and Shiny, some HTML and CSS snippets have turned out to be useful time and time again. Not being professional front-end developers, we thought it would probably be a good idea to create an easy-to-reference repository of them for future projects.\nNavigation Tabs Tabs are a great way to display information that you might want to compare side-by-side. The horizontal layout also helps to save on space. I always opt for the Bootstrap tabsets, as they are very easy and consistent to implement:\n\nTab 1  Tab 2  Tab 3     Image credit: Icons 8       Image credit: Icons 8       Image credit: Icons 8       \nTo make this:\n\u0026lt;div class=\u0026quot;container\u0026quot;\u0026gt; \u0026lt;!-- Nav tabs --\u0026gt; \u0026lt;ul class=\u0026quot;nav nav-pills nav-justified\u0026quot; role=\u0026quot;tablist\u0026quot;\u0026gt; \u0026lt;li class=\u0026quot;nav-item active\u0026quot;\u0026gt; \u0026lt;a class=\u0026quot;nav-link active\u0026quot; data-toggle=\u0026quot;tab\u0026quot; href=\u0026quot;#tab1\u0026quot;\u0026gt;\u0026lt;font size=\u0026quot;+2\u0026quot;\u0026gt;\u0026lt;b\u0026gt;Tab 1\u0026lt;/b\u0026gt;\u0026lt;/font\u0026gt;\u0026lt;/a\u0026gt; \u0026lt;/li\u0026gt; \u0026lt;li class=\u0026quot;nav-item\u0026quot;\u0026gt; \u0026lt;a class=\u0026quot;nav-link\u0026quot; data-toggle=\u0026quot;tab\u0026quot; href=\u0026quot;#tab2\u0026quot;\u0026gt;\u0026lt;font size=\u0026quot;+2\u0026quot;\u0026gt;\u0026lt;b\u0026gt;Tab 2\u0026lt;/b\u0026gt;\u0026lt;/font\u0026gt;\u0026lt;/a\u0026gt; \u0026lt;/li\u0026gt; \u0026lt;li class=\u0026quot;nav-item\u0026quot;\u0026gt; \u0026lt;a class=\u0026quot;nav-link\u0026quot; data-toggle=\u0026quot;tab\u0026quot; href=\u0026quot;#tab3\u0026quot;\u0026gt;\u0026lt;font size=\u0026quot;+2\u0026quot;\u0026gt;\u0026lt;b\u0026gt;Tab 3\u0026lt;/b\u0026gt;\u0026lt;/font\u0026gt;\u0026lt;/a\u0026gt; \u0026lt;/li\u0026gt; \u0026lt;/ul\u0026gt; \u0026lt;!-- Tab panes --\u0026gt; \u0026lt;div class=\u0026quot;tab-content\u0026quot;\u0026gt; \u0026lt;div id=\u0026quot;tab1\u0026quot; class=\u0026quot;container tab-pane active\u0026quot;\u0026gt; Tab 1 content \u0026lt;/div\u0026gt; \u0026lt;div id=\u0026quot;tab2\u0026quot; class=\u0026quot;container tab-pane fade\u0026quot;\u0026gt; Tab 2 content \u0026lt;/div\u0026gt; \u0026lt;div id=\u0026quot;tab3\u0026quot; class=\u0026quot;container tab-pane fade\u0026quot;\u0026gt; Tab 3 content \u0026lt;/div\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;/div\u0026gt; \nFloating outline When the page has a lot of content, it is very helpful to include a floating table of contents on the side of the page that shows the reader where they are. You can see an example of this on the left!\nAfter experimenting with a few options, I found that the steps outlined here by Aidan Feldman work most easily and the best for Blogdown sites:\n\u0026lt;head\u0026gt; \u0026lt;meta charset=\u0026quot;utf-8\u0026quot;\u0026gt; \u0026lt;meta name=\u0026quot;viewport\u0026quot; content=\u0026quot;width=device-width, initial-scale=1\u0026quot;\u0026gt; \u0026lt;link rel=\u0026quot;stylesheet\u0026quot; href=\u0026quot;https://cdn.rawgit.com/afeld/bootstrap-toc/v1.0.1/dist/bootstrap-toc.min.css\u0026quot; /\u0026gt; \u0026lt;script src=\u0026quot;https://ajax.googleapis.com/ajax/libs/jquery/3.4.1/jquery.min.js\u0026quot;\u0026gt;\u0026lt;/script\u0026gt; \u0026lt;script src=\u0026quot;https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.7/umd/popper.min.js\u0026quot;\u0026gt;\u0026lt;/script\u0026gt; \u0026lt;script src=\u0026quot;https://maxcdn.bootstrapcdn.com/bootstrap/4.3.1/js/bootstrap.min.js\u0026quot;\u0026gt;\u0026lt;/script\u0026gt; \u0026lt;script src=\u0026quot;https://cdn.rawgit.com/afeld/bootstrap-toc/v1.0.1/dist/bootstrap-toc.min.js\u0026quot;\u0026gt;\u0026lt;/script\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;body data-spy=\u0026quot;scroll\u0026quot; data-target=\u0026quot;#toc\u0026quot;\u0026gt; \u0026lt;div class=\u0026quot;container\u0026quot;\u0026gt; \u0026lt;div class=\u0026quot;row\u0026quot;\u0026gt; \u0026lt;!-- sidebar, which will move to the top on a small screen --\u0026gt; \u0026lt;div class=\u0026quot;col-sm-2\u0026quot;\u0026gt; \u0026lt;nav id=\u0026quot;toc\u0026quot; data-toggle=\u0026quot;toc\u0026quot; class=\u0026quot;sticky-top\u0026quot; style=\u0026#39;padding-top:40px\u0026#39;\u0026gt;\u0026lt;/nav\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;!-- main content area --\u0026gt; \u0026lt;div class=\u0026quot;col-sm-10\u0026quot;\u0026gt; \u0026lt;!-- Page content --\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;/body\u0026gt; \nIf there are headings that you do not want to be included in the outline, you can change the header HTML tag, like \u0026lt;h2\u0026gt;, to \u0026lt;h2 data-toc-skip\u0026gt;.\n\nStyling Font size The font size of specific sections of text can be altered, either by relative change:\n\u0026lt;font size=\u0026quot;+2\u0026quot;\u0026gt;This is bigger text.\u0026lt;/font\u0026gt; Or by setting the absolute size (1-7):\n\u0026lt;font size=\u0026quot;1\u0026quot;\u0026gt;This is really tiny text.\u0026lt;/font\u0026gt; \nCode blocks We love the ability to use both Python and R code in a single Rmarkdown post on Blogdown sites. With that, it would be nice to be able to easily visually distinguish Python and R code blocks. Here are some CSS stylings used on this site:\ncode{ /* Base styling for all code blocks */ padding: 3px 5px; background: #ffffff; border: 1px solid $border-color; border-radius: 3px; color: $text-color-dark; } .python { background: #ffffff; border-color: #F6B156; border-style: dotted; page-break-inside: avoid; font-family: monospace; font-size: 15px; line-height: 1.6; margin-bottom: 1.6em; max-width: 100%; overflow: auto; padding: 1em 1.5em; display: block; word-wrap: break-word; } .r { background: #ffffff; border-color: #03d944; border-style: dotted; page-break-inside: avoid; font-family: monospace; font-size: 15px; line-height: 1.6; margin-bottom: 1.6em; max-width: 100%; overflow: auto; padding: 1em 1.5em; display: block; word-wrap: break-word; }     ","permalink":"/post/neat-html-snippets-to-jazz-up-your-site/","tags":["Useful"],"title":"HTML and CSS snippets to jazz up your site"},{"categories":null,"contents":"Welcome to Intelligence Refinery!\nWe are Nancy and Mihai Chelaru-Centea, two neuroscience majors who not so long ago ditched the lab notebooks for Jupyter notebooks. With the explosion of the data science field, there is a bewildering number of articles, tutorials, Stackoverflow answers, courses and packages on the internet for anyone who cares to look. Like many other newcomers, we are navigating this ever rising sea of information, and often misinformation, with varying degrees of success each day.\nAfter having worked for some time as data scientist/developers and meeting others on the same path, we realized the importance of creating a dedicated repository in which we can continuously refine and grow our professional knowledge, so as to be able to keep pace with this fast moving field. So, we rolled up our sleeves and built this site, where we try to distill everything we are learning about data science, software development, and anything else of interest into helpful nuggets for ourselves and whoever else that may stumble onto this site.\nWe hope that you enjoy your time here, and would love to hear any comments or questions that you have!\n","permalink":"/about/about/","tags":null,"title":"About Us"}]